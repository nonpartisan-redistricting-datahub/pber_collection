{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42749c9e-eb9c-4a8b-99c4-93480fe1689f",
   "metadata": {},
   "source": [
    "## Precinct boundary file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3a86e6-7196-45a0-a503-8122d2433e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare all precinct data between 2020 and 2022:\n",
      "2022 \n",
      "ER unique_id nunique:  8933 \n",
      "shape:  (8933,) \n",
      "prec name nunique:  8760 \n",
      "prec name w county nunique 8933\n",
      "\n",
      "2020 \n",
      "PB NAME20 nunique:  8673 \n",
      "shape: (8941, 8) \n",
      "prec name nunique:  1550 \n",
      "prec name w county nunique 8941\n",
      "\n",
      "Compare subset of counties that we expect to match between 2020 and 2022:\n",
      "prec w/county diff: pb comp er len:  8\n",
      "prec w/county diff: er comp pb len:  0\n",
      "prec county diff: er comp pb names:  set()\n",
      "prec county diff: pb comp er names:  {'ZZZerie', 'ZZZottawa', 'ZZZashtabula', 'ZZZportage', 'ZZZlucas', 'ZZZlake', 'ZZZcuyahoga', 'ZZZlorain'}\n",
      "\n",
      "Compare post-merge:\n",
      "shape merged gdf:  (4375, 269) \n",
      "unmatched shape:  (8, 269)\n",
      "county from er not matching:  [nan]\n",
      "county from bound not matching, not zzz:  []\n",
      "Number of counties new to 2022//need precinct name matching:  26\n",
      "number of counties in list:  12\n",
      "check prec_id for:  CLARK\n",
      "check prec_id for:  COLUMBIANA\n",
      "check prec_id for:  DELAWARE\n",
      "check prec_id for:  GEAUGA\n",
      "check prec_id for:  HAMILTON\n",
      "check prec_id for:  HARDIN\n",
      "check prec_id for:  HOCKING\n",
      "check prec_id for:  LAKE\n",
      "check prec_id for:  MARION\n",
      "check prec_id for:  MERCER\n",
      "check prec_id for:  MUSKINGUM\n",
      "check prec_id for:  TUSCARAWAS\n",
      "number of counties in list:  12\n",
      "check prec_id for:  BROWN\n",
      "check prec_id for:  BUTLER\n",
      "check prec_id for:  CLERMONT\n",
      "check prec_id for:  CUYAHOGA\n",
      "check prec_id for:  ERIE\n",
      "check prec_id for:  MIAMI\n",
      "check prec_id for:  MONTGOMERY\n",
      "check prec_id for:  MEDINA\n",
      "check prec_id for:  LORAIN\n",
      "check prec_id for:  LUCAS\n",
      "check prec_id for:  PICKAWAY\n",
      "check prec_id for:  STARK\n",
      "number of counties in list:  1\n",
      "check prec_id for:  WOOD\n",
      "set()\n",
      "{'ZZZ'}\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import maup\n",
    "from op_verification import reference_data\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "\n",
    "#2022 precinct election results, no geospatial data\n",
    "er22 = pd.read_csv('./raw-from-source/oh_2022_gen_prec/oh_2022_gen_prec.csv')\n",
    "#New precinct boundaries since 2020 --> new precinct name matching required. \"madison\", \"huron\",\"allen\" initially in list but removed after confirming 2022 pdf map with 2020 geospatial data\n",
    "prec_counties_new22_set = {\"brown\",\"butler\",\"clark\",\"clermont\",\"columbiana\",\"cuyahoga\",\"delaware\", \"erie\",\"geauga\",\"hamilton\",\"hardin\", \n",
    "                           \"hocking\",\"lake\",\"lucas\",\"lorain\",\"marion\",\"medina\",\"mercer\",\"miami\",\"montgomery\",\"muskingum\",\"pickaway\",\"portage\",\n",
    "                           \"stark\",\"tuscarawas\",\"wood\"}\n",
    "\n",
    "\n",
    "#2020 precinct boundaries and election results file filtered for only boundaries -- 2022 boundaries below in \"Precinct name matching 2022 section\"\n",
    "bound20rdh = gp.read_file(\"./raw-from-source/oh_gen_20_prec/oh_gen_20_st_prec.shp\")[['UNIQUE_ID', \"STATEFP20\",\"COUNTYFP20\", \"PRECINCT20\",\"NAME20\",\"geometry\"]]\n",
    "#Format county/county name columns to maptch ER22 for comparison\n",
    "bound20rdh['COUNTY'] = (\"39\"+bound20rdh['COUNTYFP20']).map(reference_data.geoid_to_county_name)\n",
    "bound20rdh['COUNTYNM'] = bound20rdh['COUNTY'].str.lower().str.slice(stop=-7)\n",
    "county_name_to_fips_dict = pd.Series(bound20rdh['COUNTYFP20'].values, index=bound20rdh['COUNTYNM']).to_dict()\n",
    "print(\"Compare all precinct data between 2020 and 2022:\")\n",
    "#Confirm county names match up\n",
    "assert set(bound20rdh['COUNTYNM'])-set(er22[\"County\"].str.lower()) == set()\n",
    "assert set(er22[\"County\"].str.lower())-set(bound20rdh['COUNTYNM']) == set()\n",
    "#Considering 2020/2022 comparison some more\n",
    "print(\"2022 \\nER unique_id nunique: \",er22[\"UNIQUE_ID\"].nunique() ,\"\\nshape: \", er22[\"UNIQUE_ID\"].shape, \"\\nprec name nunique: \", er22[\"PRECNAME\"].nunique(), \"\\nprec name w county nunique\",(er22[\"PRECCODE\"]+er22[\"County\"]).nunique())\n",
    "print(\"\\n2020 \\nPB NAME20 nunique: \",bound20rdh[\"NAME20\"].nunique(),\"\\nshape:\",bound20rdh.shape, \"\\nprec name nunique: \",bound20rdh[\"PRECINCT20\"].nunique(),\"\\nprec name w county nunique\",(bound20rdh[\"PRECINCT20\"]+bound20rdh[\"COUNTYNM\"]).nunique())\n",
    "\n",
    "print(\"\\nCompare subset of counties that we expect to match between 2020 and 2022:\")\n",
    "#set of counties with reused precinct boundaries from pre-2020, therefore can use 2020 geospatial data\n",
    "counties_reused22_set = set(er22[\"County\"].str.lower())-prec_counties_new22_set\n",
    "#2022 ER df subset counties where geospatial data can be reused\n",
    "er22_reused_counties_df = er22[er22[\"County\"].str.lower().isin(counties_reused22_set)]\n",
    "#2020 Bound gdf subset counties where geospatial data can be reused\n",
    "bound20_reused_counties_gdf = bound20rdh[(bound20rdh[\"COUNTYNM\"].isin(counties_reused22_set))|(bound20rdh[\"PRECINCT20\"]==\"ZZZ\")]\n",
    "#Check if Precincts line up/what mismatches - as expected, only for ZZZ 0 voter precicnts\n",
    "print(\"prec w/county diff: pb comp er len: \", len(set(bound20_reused_counties_gdf[\"PRECINCT20\"]+bound20_reused_counties_gdf[\"COUNTYNM\"].str.lower())-set(er22_reused_counties_df[\"PRECCODE\"]+er22_reused_counties_df[\"County\"].str.lower())))\n",
    "print(\"prec w/county diff: er comp pb len: \", len(set(er22_reused_counties_df[\"PRECCODE\"]+er22_reused_counties_df[\"County\"].str.lower())-set(bound20_reused_counties_gdf[\"PRECINCT20\"]+bound20_reused_counties_gdf[\"COUNTYNM\"])))\n",
    "print(\"prec county diff: er comp pb names: \", set(er22_reused_counties_df[\"PRECCODE\"]+er22_reused_counties_df[\"County\"].str.lower())-set(bound20_reused_counties_gdf[\"PRECINCT20\"]+bound20_reused_counties_gdf[\"COUNTYNM\"]))\n",
    "print(\"prec county diff: pb comp er names: \", set(bound20_reused_counties_gdf[\"PRECINCT20\"]+bound20_reused_counties_gdf[\"COUNTYNM\"].str.lower())-set(er22_reused_counties_df[\"PRECCODE\"]+er22_reused_counties_df[\"County\"].str.lower()))\n",
    "\n",
    "#Create common column to join on\n",
    "er22_reused_counties_df[\"UNIQUE_ID_code\"] = er22_reused_counties_df[\"County\"].str.upper() + \"-\"+er22_reused_counties_df[\"PRECCODE\"]\n",
    "bound20_reused_counties_gdf[\"UNIQUE_ID_code\"] = bound20_reused_counties_gdf[\"COUNTYNM\"].str.upper()+\"-\"+bound20_reused_counties_gdf[\"PRECINCT20\"]\n",
    "#Join 2020 bounds with 2022 ER where appropriate\n",
    "pber_prec_reused_gdf = pd.merge(er22_reused_counties_df, bound20_reused_counties_gdf, on = \"UNIQUE_ID_code\", how = \"outer\", indicator=True)\n",
    "print(\"\\nCompare post-merge:\")\n",
    "print(\"shape merged gdf: \",pber_prec_reused_gdf.shape, \"\\nunmatched shape: \",pber_prec_reused_gdf[pber_prec_reused_gdf[\"_merge\"]!=\"both\"].shape)\n",
    "print(\"county from er not matching: \",pber_prec_reused_gdf[\"County\"][pber_prec_reused_gdf[\"_merge\"]!=\"both\"].unique())\n",
    "print(\"county from bound not matching, not zzz: \",pber_prec_reused_gdf[\"COUNTYNM\"][(pber_prec_reused_gdf[\"_merge\"]!=\"both\")&(pber_prec_reused_gdf[\"PRECINCT20\"]!=\"ZZZ\")].unique())\n",
    "\n",
    "\n",
    "def check_precid_uniqueness(county_gdf_list):\n",
    "    '''\n",
    "    Checks that the \"prec_id\" column created for each county gdf is unique within the county and that it matches the precinct identifiers in the election results df. \n",
    "    If it is not unique or does not match, function will return and assertion error\n",
    "    Parameter county_df_list: list of dataframes where each dataframe is one county to be considered \n",
    "    '''\n",
    "    print(\"number of counties in list: \",len(county_gdf_list))\n",
    "    for county in county_gdf_list:\n",
    "        print(\"check prec_id for: \", county[\"county\"][0])\n",
    "        assert county[\"prec_id\"].nunique()==len(county)\n",
    "        assert set(county[\"prec_id\"])-set(er22[\"PRECNAME\"][er22[\"County\"].str.upper()==county[\"county\"][0]])==set(er22[\"PRECNAME\"][er22[\"County\"].str.upper()==county[\"county\"][0]])-set(county[\"prec_id\"])==set()\n",
    "    \n",
    "    return \"all counties pass check\"\n",
    "\n",
    "\n",
    "print(\"Number of counties new to 2022//need precinct name matching: \",len(prec_counties_new22_set))\n",
    "\n",
    "\n",
    "#Clark: Full match\n",
    "clark = gp.read_file(\"./raw-from-source/boundaries/clark_precincts_2023/ClarkCountyPrecinctData.shp\")\n",
    "clark['prec_id'] = \"PRECINCT \"+clark['NAME']\n",
    "clark.loc[clark['NAME'].str.contains(\"BETHEL\"), \"prec_id\"] = clark['prec_id'].str.replace(\"BETHEL\", \"BETH\")\n",
    "clark.loc[clark['NAME'].str.contains(\" 0\"), \"prec_id\"] = clark['prec_id'].str.replace(\" 0\", \" \")\n",
    "clark.loc[clark['NAME'].str.contains(\"MR \"), \"prec_id\"] = clark['prec_id'].str.replace(\"MR \", \"MR-\")\n",
    "clark.loc[clark['NAME'].str.contains(\"HARMONY\"), \"prec_id\"] = clark['prec_id'].str.replace(\"HARMONY\", \"HARM\")\n",
    "clark.loc[clark['NAME'].str.contains(\"T-0\"), \"prec_id\"] = clark['prec_id'].str.replace(\"T-0\", \"T-\")\n",
    "clark.loc[clark['NAME'].str.contains(\"GERMAN\"), \"prec_id\"] = clark['prec_id'].str.replace(\"GERMAN\", \"GERM\")\n",
    "clark.loc[clark['NAME'].str.contains(\"GREEN\"), \"prec_id\"] = clark['prec_id'].str.replace(\"GREEN\", \"GREE\")\n",
    "clark['county'] = 'CLARK'\n",
    "clark['UNIQUE_ID'] = clark['county']+\"-\"+clark[\"prec_id\"]\n",
    "clark = clark.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Columbiana: Full match\n",
    "columbiana = gp.read_file(\"./raw-from-source/boundaries/columbiana_precincts_2022/Voting_Precincts_2022.shp\")\n",
    "columbiana_dict = {\"HANOVER TWP NW\":'PRECINCT HANOVER TWP NORTH',\"WELLSVILLE 2\":'PRECINCT WELLSVILLE  2',\"PERRY SW\":'PRECINCT PERRY TWP SW',\n",
    "                   \"PERRY NORTH\":'PRECINCT PERRY TWP NORTH',\"LEETONIA\":'PRECINCT LEETONIA VIL', \"NEGLEY\":'PRECINCT MIDDLETON TWP NEGLEY',\n",
    "                   \"ROGERS\":'PRECINCT MIDDLETON TWP ROGERS',\"ST CLAIR SOUTH\":'PRECINCT ST CLAIR TWP SOUTH',\n",
    "                   \"WEST TWP E ROCHESTER\":'PRECINCT WEST TWP EAST ROCHESTER',\"WELLSVILLE 1\":'PRECINCT WELLSVILLE  1'}\n",
    "columbiana[\"prec_id\"] = \"PRECINCT \"+columbiana[\"PRECNAME\"].str.upper().str.replace(\"TOWNSHIP\", \"TWP\")\n",
    "columbiana.loc[columbiana[\"PRECNAME\"].isin(columbiana_dict.keys()), \"prec_id\"] = columbiana[\"PRECNAME\"].map(columbiana_dict)\n",
    "columbiana[\"county\"] = \"COLUMBIANA\"\n",
    "columbiana[\"UNIQUE_ID\"] = columbiana[\"county\"]+\"-\"+columbiana[\"prec_id\"]\n",
    "columbiana = columbiana.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Delaware: Full match\n",
    "delaware = gp.read_file(\"./raw-from-source/boundaries/delaware_precincts_2022/Precinct.shp\")\n",
    "delaware['prec_id'] = \"PRECINCT \" + delaware['PREC_NAME'].str.upper()\n",
    "delaware[\"county\"] = \"DELAWARE\"\n",
    "delaware.loc[delaware[\"PREC_NAME\"].str.contains(\"COLUMBUS\"), \"prec_id\"] = \"PRECINCT \"+delaware[\"PREC_NAME\"].str.replace(\"COLUMBUS \", \"COLUMBUS CITY \")\n",
    "delaware.loc[delaware[\"PREC_NAME\"].str.contains(\"DUBLIN\"), \"prec_id\"] = \"PRECINCT \"+delaware[\"PREC_NAME\"].str.replace(\"DUBLIN \", \"DUBLIN CITY \")\n",
    "delaware.loc[delaware[\"PREC_NAME\"].str.contains(\"POWELL\"), \"prec_id\"] = \"PRECINCT \"+delaware[\"PREC_NAME\"].str.replace(\"POWELL \", \"POWELL CITY \")\n",
    "delaware.loc[delaware[\"PREC_NAME\"].str.contains(\"SUNBURY\"), \"prec_id\"] = \"PRECINCT \"+delaware[\"PREC_NAME\"].str.replace(\"SUNBURY \", \"SUNBURY CITY \")\n",
    "delaware.loc[delaware[\"PREC_NAME\"].str.contains(\"WESTERVILLE\"), \"prec_id\"] = \"PRECINCT \"+delaware[\"PREC_NAME\"].str.replace(\"WESTERVILLE \", \"WESTERVILLE CITY \")\n",
    "delaware[\"UNIQUE_ID\"] = delaware[\"county\"]+\"-\"+delaware[\"prec_id\"]\n",
    "delaware = delaware.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Geauga: Full Match\n",
    "geauga = gp.read_file(\"./raw-from-source/boundaries/geauga_precincts_2022/Precincts.shp\")\n",
    "geauga_prec_dict = {\"B-76\":\"PRECINCT THOMPSON TWP B\",\"B-51\":\"PRECINCT MONTVILLE TWP B\", \"C-42\":\"PRECINCT HAMBDEN TWP C\", \"A-23\":\"PRECINCT CHARDON TWP A\",\"D-22\":\"PRECINCT CHARDON CITY D\", \"A-44\":\"PRECINCT HUNTSBURG TWP A\", \"B-38\":\"PRECINCT CLARIDON TWP B\", \"E-56\":\"PRECINCT MUNSON TWP E\",\n",
    "\"D-30\":\"PRECINCT CHESTER TWP D\",\"AQV-36\":\"PRECINCT AQUILLA VILLAGE\", \"B-49\":\"PRECINCT MIDDLEFIELD TWP B\", \"C-18\":\"PRECINCT BURTON TWP C\", \"A-58\":\"PRECINCT NEWBURY TWP A\", \"A-70\":\"PRECINCT RUSSELL TWP A\", \"HVV-69\":\"PRECINCT HUNTING VALLEY VILL\", \n",
    "\"B-47\":\"PRECINCT MIDDLEFIELD VILL B\",\n",
    "\"BUR V-15\":\"PRECINCT BURTON VILLAGE\", \"B-64\":\"PRECINCT PARKMAN TWP B\", \"A-77\":\"PRECINCT TROY TWP A\", \"E-5\":\"PRECINCT AUBURN TWP E\",\"F-11\":\"PRECINCT BAINBRIDGE TWP F\", \"A-75\":\"PRECINCT THOMPSON TWP A\", \"A-50\":\"PRECINCT MONTVILLE TWP A\", \"B-45\":\"PRECINCT HUNTSBURG TWP B\", \n",
    "\"A-46\":\"PRECINCT MIDDLEFIELD VILL A\", \"A-63\":\"PRECINCT PARKMAN TWP A\", \"B-78\":\"PRECINCT TROY TWP B\", \"A-16\":\"PRECINCT BURTON TWP A\", \"B-17\":\"PRECINCT BURTON TWP B\", \"A-37\":\"PRECINCT CLARIDON TWP A\", \"B-41\":\"PRECINCT HAMBDEN TWP B\", \"A-40\":\"PRECINCT HAMBDEN TWP A\",\n",
    "\"D-43\":\"PRECINCT HAMBDEN TWP D\", \"C-25\":\"PRECINCT CHARDON TWP C\",\"D-26\":\"PRECINCT CHARDON TWP D\", \"B-24\":\"PRECINCT CHARDON TWP B\", \"A-19\":\"PRECINCT CHARDON CITY A\", \"C-21\":\"PRECINCT CHARDON CITY C\", \"B-20\":\"PRECINCT CHARDON CITY B\", \"B-53\":\"PRECINCT MUNSON TWP B\", \n",
    "\"C-54\":\"PRECINCT MUNSON TWP C\",\"F-57\":\"PRECINCT MUNSON TWP F\", \"B-59\":\"PRECINCT NEWBURY TWP B\", \"C-60\":\"PRECINCT NEWBURY TWP C\", \"E-62\":\"PRECINCT NEWBURY TWP E\",\"D-61\":\"PRECINCT NEWBURY TWP D\", \"C-3\":\"PRECINCT AUBURN TWP C\", \"B-2\":\"PRECINCT AUBURN TWP B\", \n",
    "\"A-1\":\"PRECINCT AUBURN TWP A\",\"D-4\":\"PRECINCT AUBURN TWP D\", \"A-6\":\"PRECINCT BAINBRIDGE TWP A\", \"C-8\":\"PRECINCT BAINBRIDGE TWP C\", \"E-10\":\"PRECINCT BAINBRIDGE TWP E\",\"H-13\":\"PRECINCT BAINBRIDGE TWP H\",\"I-14\":\"PRECINCT BAINBRIDGE TWP I\",\"D-9\":\"PRECINCT BAINBRIDGE TWP D\",\n",
    "\"D-68\":\"PRECINCT SOUTH RUSSELL VILL D\", \"B-66\":\"PRECINCT SOUTH RUSSELL VILL B\", \"A-65\":\"PRECINCT SOUTH RUSSELL VILL A\", \"C-67\":\"PRECINCT SOUTH RUSSELL VILL C\", \"E-74\":\"PRECINCT RUSSELL TWP E\", \"C-72\":\"PRECINCT RUSSELL TWP C\", \"B-71\":\"PRECINCT RUSSELL TWP B\",\n",
    "\"D-73\":\"PRECINCT RUSSELL TWP D\", \"C-29\":\"PRECINCT CHESTER TWP C\",\"G-33\":\"PRECINCT CHESTER TWP G\",\"H-34\":\"PRECINCT CHESTER TWP H\",\"I-35\":\"PRECINCT CHESTER TWP I\", \"A-27\":\"PRECINCT CHESTER TWP A\", \"B-28\":\"PRECINCT CHESTER TWP B\", \"E-31\":\"PRECINCT CHESTER TWP E\",\n",
    "\"F-32\":\"PRECINCT CHESTER TWP F\", \"B-7\":\"PRECINCT BAINBRIDGE TWP B\",\"G-12\":\"PRECINCT BAINBRIDGE TWP G\",\"D-55\":\"PRECINCT MUNSON TWP D\", \"A-52\":\"PRECINCT MUNSON TWP A\", \"C-39\":\"PRECINCT CLARIDON TWP C\", \"A-48\":\"PRECINCT MIDDLEFIELD TWP A\"}\n",
    "geauga[\"prec_id\"] = \"na\"\n",
    "geauga.loc[geauga['PRECINCT2'].isin(geauga_prec_dict.keys()), \"prec_id\"] = geauga[\"PRECINCT2\"].map(geauga_prec_dict)\n",
    "geauga[\"county\"] = \"GEAUGA\"\n",
    "geauga[\"UNIQUE_ID\"] = geauga[\"county\"]+\"-\"+geauga[\"prec_id\"]\n",
    "geauga = geauga.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Hamilton: Full Match\n",
    "hamilton = gp.read_file(\"./raw-from-source/boundaries/hamilton_precincts_2022/PRECINT2021_0311.shp\")\n",
    "hamilton['prec_id'] = hamilton[\"PRC_NAME\"]\n",
    "hamilton['county'] = \"HAMILTON\"\n",
    "hamilton[\"UNIQUE_ID\"] = hamilton[\"county\"]+\"-\"+hamilton[\"prec_id\"]\n",
    "hamilton = hamilton.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Hardin: Full Match\n",
    "hardin = gp.read_file(\"./raw-from-source/boundaries/hardin_precincts_2022/Hardin_Co_Precincts_2023-04.shp\")\n",
    "hardin[\"prec_id\"] = hardin[\"precinct\"].str.upper()\n",
    "hardin[\"county\"] = \"HARDIN\"\n",
    "hardin[\"UNIQUE_ID\"] = hardin[\"county\"]+\"-\"+hardin[\"prec_id\"]\n",
    "hardin = hardin.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Hocking: Full Match\n",
    "hocking = gp.read_file(\"./raw-from-source/boundaries/hocking_precincts_2022/HOCKING_COUNTY_PRECINCTS_2022.shp\")\n",
    "hocking[\"prec_id\"] = \"PRECINCT \" + hocking[\"PRECINCT\"]\n",
    "hocking.loc[hocking[\"PRECINCT\"]==\"SALT CREEK\", \"prec_id\"] = \"PRECINCT SALTCREEK\"\n",
    "hocking[\"county\"] = \"HOCKING\"\n",
    "hocking[\"UNIQUE_ID\"] = hocking[\"county\"]+\"-\"+hocking[\"prec_id\"]\n",
    "hocking = hocking.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Lake: Full Match\n",
    "lake = gp.read_file(\"./raw-from-source/boundaries/lake_precincts_2021/Lake_County_Voting_Precincts_(2021).shp\")\n",
    "lake[\"prec_id\"] = lake[\"NAME\"].str.upper()\n",
    "lake.loc[~lake[\"NAME\"].str.upper().str.contains(\"MENTOR-ON\"), \"prec_id\"] = \"PRECINCT \"+lake[\"NAME\"].str.upper()\n",
    "lake.loc[lake[\"NAME\"].str.upper().str.contains(\"MENTOR-ON\"), \"prec_id\"] = lake[\"NAME\"].str.upper().str.slice(stop=-2)+\"CITY \"+lake[\"NAME\"].str.upper().str.slice(start=-2)\n",
    "lake[\"county\"] = \"LAKE\"\n",
    "lake[\"UNIQUE_ID\"] = lake[\"county\"]+\"-\"+lake[\"prec_id\"]\n",
    "lake = lake.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Marion: Full Match\n",
    "marion_aug=gp.read_file(\"./raw-from-source/boundaries/marion_precincts_2022/MARION_COUNTY_PRECINCTS_CONSOLIDATED_AUG2022.shp\")\n",
    "marion_dec = gp.read_file(\"./raw-from-source/boundaries/marion_precincts_2022/City_Council_Wards_December2022.shp\")\n",
    "marion_dec[\"PRCT_NAME\"] = \"MARION \" + marion_dec[\"WARD_ID\"].astype(str)+\"-\"+marion_dec[\"Prect\"]\n",
    "marion = gp.GeoDataFrame(pd.concat([marion_aug, marion_dec], ignore_index=True), crs=bound20_reused_counties_gdf.crs)\n",
    "marion[\"prec_id\"] = \"PRECINCT \"+marion[\"PRCT_NAME\"]\n",
    "marion_dict = {'PRECINCT BIG ISLAND':'PRECINCT BIG ISLAND TWP','PRECINCT BOWLING GREEN':'PRECINCT BOWLING GREEN TWP',\n",
    "               'PRECINCT CLARIDON A':'PRECINCT CLARIDON TWP A','PRECINCT CLARIDON B':'PRECINCT CLARIDON TWP B',\n",
    "               'PRECINCT GRAND PRAIRIE':'PRECINCT GRAND PRAIRIE TWP','PRECINCT GRAND-SALT':'PRECINCT GRAND SALT',\n",
    "               'PRECINCT GREEN CAMP':'PRECINCT GREEN CAMP TWP','PRECINCT MARION A':'PRECINCT MARION TWP A','PRECINCT MARION B':'PRECINCT MARION TWP B',\n",
    "               'PRECINCT MARION C':'PRECINCT MARION TWP C','PRECINCT MARION D':'PRECINCT MARION TWP D','PRECINCT MARION E':'PRECINCT MARION TWP E',\n",
    "               'PRECINCT MARION F':'PRECINCT MARION TWP F','PRECINCT MONTGOMERY':'PRECINCT MONTGOMERY TWP','PRECINCT PLEASANT A':'PRECINCT PLEASANT TWP A',\n",
    "               'PRECINCT PLEASANT B':'PRECINCT PLEASANT TWP B','PRECINCT PLEASANT C':'PRECINCT PLEASANT TWP C','PRECINCT PLEASANT D':'PRECINCT PLEASANT TWP D',\n",
    "               'PRECINCT PROSPECT':'PRECINCT PROSPECT TWP','PRECINCT RICHLAND A':'PRECINCT RICHLAND TWP A','PRECINCT RICHLAND B':'PRECINCT RICHLAND TWP B',\n",
    "               'PRECINCT SCOTT-TULLY':'PRECINCT SCOTT TULLY','PRECINCT WALDO':'PRECINCT WALDO TWP'}\n",
    "marion.loc[marion[\"prec_id\"].isin(marion_dict), \"prec_id\"] = marion[\"prec_id\"].map(marion_dict)\n",
    "marion[\"county\"] = \"MARION\"\n",
    "marion[\"UNIQUE_ID\"] = marion[\"county\"]+\"-\"+marion[\"prec_id\"]\n",
    "marion = marion.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Mercer: Full Match\n",
    "mercer = gp.read_file(\"./raw-from-source/boundaries/mercer_precincts_2023/2023_03_votingprecincts.shp\")\n",
    "mercer[\"prec_id\"] = mercer[\"NAME\"]\n",
    "mercer[\"county\"] = \"MERCER\"\n",
    "mercer[\"UNIQUE_ID\"] = mercer[\"county\"]+\"-\"+mercer[\"prec_id\"]\n",
    "mercer = mercer.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Muskingum: Full match\n",
    "muskingum = gp.read_file(\"./raw-from-source/boundaries/muskingum_precincts_2022/VOTING_PRECINCTS.shp\")\n",
    "muskingum_dict = {'1A': 'PRECINCT ZANESVILLE 1-A', '1B': 'PRECINCT ZANESVILLE 1-B', '2A': 'PRECINCT ZANESVILLE 2-A', '2B': 'PRECINCT ZANESVILLE 2-B', '3A': 'PRECINCT ZANESVILLE 3-A', '3B': 'PRECINCT ZANESVILLE 3-B', \n",
    "                  '4A': 'PRECINCT ZANESVILLE 4-A', '4B': 'PRECINCT ZANESVILLE 4-B', '5A': 'PRECINCT ZANESVILLE 5-A', '5B': 'PRECINCT ZANESVILLE 5-B', '5C': 'PRECINCT ZANESVILLE 5-C', '6A': 'PRECINCT ZANESVILLE 6-A', \n",
    "                  '6B': 'PRECINCT ZANESVILLE 6-B', '6C': 'PRECINCT ZANESVILLE 6-C', 'NEWTON FULTONHAM': 'PRECINCT FULTONHAM', 'NEWTON IRONSPT': 'PRECINCT IRONSPOT', 'NEWTON MOXAHALA': 'PRECINCT MOXAHALA', \n",
    "                  'NEWTON ROLLING PLAINS': 'PRECINCT ROLLING PLAINS', 'NEWTON WHITE COTTAGE': 'PRECINCT WHITE COTTAGE', 'UNION UNION': 'PRECINCT UNION', 'WAYNE DUNCAN FALLS':'PRECINCT DUNCAN FALLS'}\n",
    "muskingum[\"prec_id\"] = \"PRECINCT \"+muskingum[\"PRECINCT\"]\n",
    "muskingum.loc[muskingum[\"PRECINCT\"].isin(muskingum_dict.keys()), \"prec_id\"] = muskingum[\"PRECINCT\"].map(muskingum_dict)\n",
    "muskingum[\"county\"] = \"MUSKINGUM\"\n",
    "muskingum[\"UNIQUE_ID\"] = muskingum[\"county\"]+\"-\"+muskingum[\"prec_id\"]\n",
    "muskingum = muskingum.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Tuscarawas: Full match\n",
    "tuscarawas = gp.read_file(\"./raw-from-source/boundaries/tuscarawas_precincts_2020_2022/BOE_Precincts_2022/Tusc157_BOE_PctBnd.shp\")\n",
    "tuscarawas_dict_df = pd.read_csv(\"./raw-from-source/boundaries/tuscarawas_precincts_2020_2022/BOE_Precincts_2022/prec_matching.csv\")\n",
    "tuscarawas_dict = pd.Series(tuscarawas_dict_df[\"ER\"].str.replace(\"'\",\"\").str.replace(\",\",\"\").str.slice(start=1).values, index = tuscarawas_dict_df[\"PB\"].str.replace(\"'\",\"\").str.replace(\",\",\"\").str.slice(start=1)).to_dict()\n",
    "tuscarawas[\"prec_id\"] = \"PRECINCT \"+tuscarawas[\"PRECT_NA\"]\n",
    "tuscarawas.loc[tuscarawas[\"prec_id\"].isin(tuscarawas_dict.keys()), \"prec_id\"] = tuscarawas[\"prec_id\"].map(tuscarawas_dict)\n",
    "tuscarawas[\"county\"] = \"TUSCARAWAS\"\n",
    "tuscarawas[\"UNIQUE_ID\"] = tuscarawas[\"county\"]+\"-\"+tuscarawas[\"prec_id\"]\n",
    "tuscarawas = tuscarawas.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Check if uniqueness/match fixed\n",
    "full_match_22_bound=[clark, columbiana, delaware, geauga, hamilton, hardin, hocking, lake, marion,mercer, muskingum, tuscarawas]\n",
    "check_precid_uniqueness(full_match_22_bound) \n",
    "\n",
    "\n",
    "#Brown: Full match \n",
    "brown = gp.read_file(\"./raw-from-source/boundaries/brown_precincts_2023/VOTING-PRECINCT.shp\") #do the overlap labels have overlapping shapes also?\n",
    "brown['prec_id'] = brown['NAME']\n",
    "brown[\"county\"] = \"BROWN\"\n",
    "brown.loc[brown['NAME']==\"ABERDEEN\", \"prec_id\"] = \"ABERDEEN VILLAGE\"\n",
    "brown.loc[brown['NAME']==\"LEWIS HIGGINSPORT\", \"prec_id\"] = \"LEWIS/HIGGINSPORT\"\n",
    "brown.loc[brown['NAME']==\"MT ORAB STERLING\", \"prec_id\"] = \"MOUNT ORAB VILLAGE WEST\"\n",
    "brown.loc[brown['NAME']==\"PERRY LAKE LORELEI\", \"prec_id\"] = \"PERRY / LAKE LORELEI\"\n",
    "brown.loc[brown['NAME']==\"PERRY TWP VILLAGES\", \"prec_id\"] = \"PERRY TWP-VILLAGES\"\n",
    "brown.loc[brown['NAME']==\"SARDINIA\", \"prec_id\"] = \"SARDINIA VILLAGE\"\n",
    "brown[\"UNIQUE_ID\"] = brown[\"county\"] + \"-\" + brown['prec_id']\n",
    "brown = brown.dissolve(by = \"UNIQUE_ID\").reset_index()\n",
    "brown = brown.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Butler: Full match, combine prec\n",
    "butler = gp.read_file(\"./raw-from-source/boundaries/butler_precincts_2023/2023Precincts.shp\")\n",
    "butler_dict_df = pd.read_csv(\"./raw-from-source/boundaries/butler_precincts_2023/prec_name_matching.csv\")\n",
    "butler['prec_id'] = butler['NEW_PREC_N'].str.upper().str.replace(\"FFTWP\", \"FAIRFIELD TWP \").str.replace(\"HANOVER\", \"HANOVER TWP \").str.replace(\n",
    "    \"LEMON\", \"LEMON TWP \").str.replace(\"LIBERTY\", \"LIBERTY TWP \").str.replace(\"WC\", \"WEST CHESTER TWP \")\n",
    "butler_dict = pd.Series(butler_dict_df[\"ER\"].str.replace(\"'\",\"\").str.replace(\",\",\"\").str.slice(start=1).values, index = butler_dict_df[\"PB\"].str.replace(\"'\",\"\").str.replace(\",\",\"\").str.slice(start=1).values).to_dict()\n",
    "butler.loc[butler[\"prec_id\"].isin(butler_dict.keys()), \"prec_id\"] = butler[\"prec_id\"].map(butler_dict)\n",
    "#Dissolve\n",
    "butler = butler.dissolve(by=\"prec_id\").reset_index()\n",
    "butler[\"county\"] = \"BUTLER\"\n",
    "butler[\"UNIQUE_ID\"] = butler[\"county\"]+\"-\"+butler[\"prec_id\"]\n",
    "butler = butler.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Clermont: Full match\n",
    "clermont = gp.read_file(\"./raw-from-source/boundaries/clermont_precincts_2023/VotingPrecincts.shp\")\n",
    "clermont['prec_id'] = clermont[\"PRECINCT\"].str.upper().str.replace(\"TWP\",\"TOWNSHIP\")\n",
    "clermont['county'] = \"CLERMONT\"\n",
    "clermont[\"UNIQUE_ID\"] = clermont['county']+\"-\"+clermont[\"prec_id\"]\n",
    "clermont = clermont.dissolve(by=\"UNIQUE_ID\").reset_index()\n",
    "clermont = clermont.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Cuyahoga: Full match, combine prec\n",
    "cuyahoga = gp.read_file(\"./raw-from-source/boundaries/cuyahoga_precincts_2022/Precincts May 2022_region.shp\")\n",
    "cuyahoga_dict_df = pd.read_csv(\"./raw-from-source/boundaries/cuyahoga_precincts_2022/prec_matching.csv\")\n",
    "cuyahoga_dict = pd.Series(cuyahoga_dict_df[\"ER\"].str.replace(\"'\",\"\").str.replace(\",\",\"\").str.slice(start=1).values, index=cuyahoga_dict_df[\"PB\"].str.replace(\"'\",\"\").str.replace(\",\",\"\").str.slice(start=1)).to_dict()\n",
    "cuyahoga['prec_id'] = cuyahoga[\"City\"].str.upper()+\"-\"+cuyahoga['Label'].str.slice(stop=-1).str.zfill(2)+\"-\"+cuyahoga['Label'].str.slice(start=-1)\n",
    "cuyahoga.loc[cuyahoga[\"prec_id\"].isin(cuyahoga_dict.keys()), \"prec_id\"] = cuyahoga[\"prec_id\"].map(cuyahoga_dict)\n",
    "cuyahoga.loc[cuyahoga[\"prec_id\"]=='EACHWOOD-00-E', \"prec_id\"] = 'BEACHWOOD-00-E'\n",
    "#Buffer small area, otherwise dissolve fails\n",
    "cuyahoga[\"geometry\"] = cuyahoga[\"geometry\"].buffer(0.01)\n",
    "cuyahoga = cuyahoga.dissolve(by=\"prec_id\").reset_index()\n",
    "cuyahoga['county'] = \"CUYAHOGA\"\n",
    "cuyahoga[\"UNIQUE_ID\"] = cuyahoga[\"county\"]+\"-\"+cuyahoga[\"prec_id\"]\n",
    "cuyahoga = cuyahoga.to_crs(bound20_reused_counties_gdf.crs)\n",
    "#print(\"# prec in PB not in ER: \",len(set(cuyahoga[\"prec_id\"])-set(er22[\"PRECNAME\"][er22[\"County\"]=='Cuyahoga'])))\n",
    "#print(\"# prec in ER not in PB: \",len(set(er22[\"PRECNAME\"][er22[\"County\"]=='Cuyahoga'])-set(cuyahoga[\"prec_id\"])))\n",
    "#print(\"nunique pb: \", cuyahoga[\"prec_id\"].nunique(),\"\\nshape PB: \", cuyahoga.shape)\n",
    "#print(\"nunique er: \",er22[\"PRECNAME\"][er22[\"County\"]=='Cuyahoga'].nunique())\n",
    "\n",
    "\n",
    "#Erie: Full match except  {'Bellevue City-Annexed-Refer to Board of Elections'}\n",
    "erie = gp.read_file(\"./raw-from-source/boundaries/erie_precincts_2022/Erie_County_Voting_Precincts.shp\")\n",
    "erie[\"prec_id\"] = erie[\"Precinct_I\"]\n",
    "erie_prec_dict = {'Berlin Twp 1': 'PRECINCT BER TWP #1','Berlin Twp 2': 'PRECINCT BER TWP #2','Berlin Heights Village': 'PRECINCT BER VILL',\n",
    "                  'Bay View Village': 'PRECINCT BV VILL','Castalia Village': 'PRECINCT CAST VILL','Florence Twp 1': 'PRECINCT FLO TWP #1',\n",
    "                  'Florence Twp 2': 'PRECINCT FLO TWP #2','Groton Twp': 'PRECINCT GRO TWP','HUR-A': 'PRECINCT HUR A','HUR-B': 'PRECINCT HUR B',\n",
    "                  'HUR-C': 'PRECINCT HUR C','HUR-D': 'PRECINCT HUR D','HUR-E': 'PRECINCT HUR E','HUR-F': 'PRECINCT HUR F','Huron Twp 1': 'PRECINCT HUR TWP #1',\n",
    "                  'Huron Twp 2': 'PRECINCT HUR TWP #2','Huron Twp 3': 'PRECINCT HUR TWP #3','Kelleys Island Village': 'PRECINCT KI VILL',\n",
    "                  'Margaretta Twp 1': 'PRECINCT MAR TWP #1','Margaretta Twp 2': 'PRECINCT MAR TWP #2','Margaretta Twp 3': 'PRECINCT MAR TWP #3',\n",
    "                  'Margaretta Twp 4': 'PRECINCT MAR TWP #4','Milan Twp 1': 'PRECINCT MIL TWP #1','Milan Twp 2': 'PRECINCT MIL TWP #2',\n",
    "                  'Milan Village': 'PRECINCT MIL VILL','Oxford Twp': 'PRECINCT OX TWP','Perkins Twp 1': 'PRECINCT PER TWP #1',\n",
    "                  'Perkins Twp 10': 'PRECINCT PER TWP #10','Perkins Twp 2': 'PRECINCT PER TWP #2','Perkins Twp 3': 'PRECINCT PER TWP #3',\n",
    "                  'Perkins Twp 4': 'PRECINCT PER TWP #4','Perkins Twp 5': 'PRECINCT PER TWP #5','Perkins Twp 6': 'PRECINCT PER TWP #6',\n",
    "                  'Perkins Twp 7': 'PRECINCT PER TWP #7','Perkins Twp 8': 'PRECINCT PER TWP #8','Perkins Twp 9': 'PRECINCT PER TWP #9',\n",
    "                  'SAN-A': 'PRECINCT SAN A','SAN-B': 'PRECINCT SAN B','SAN-C': 'PRECINCT SAN C','SAN-D': 'PRECINCT SAN D','SAN-E': 'PRECINCT SAN E',\n",
    "                  'SAN-F': 'PRECINCT SAN F','SAN-G': 'PRECINCT SAN G','SAN-H': 'PRECINCT SAN H','SAN-I': 'PRECINCT SAN I','SAN-J': 'PRECINCT SAN J',\n",
    "                  'SAN-K': 'PRECINCT SAN K','SAN-L': 'PRECINCT SAN L','SAN-M': 'PRECINCT SAN M','SAN-N': 'PRECINCT SAN N','SAN-O': 'PRECINCT SAN O',\n",
    "                  'SAN-P': 'PRECINCT SAN P','Vermilion 1-A': 'PRECINCT VER 1-A','Vermilion 2-A': 'PRECINCT VER 2-A','Vermilion 2-B': 'PRECINCT VER 2-B',\n",
    "                  'Vermilion 3-A': 'PRECINCT VER 3-A','Vermilion 3-B': 'PRECINCT VER 3-B','Vermilion Twp 1': 'PRECINCT VER TWP #1',\n",
    "                  'Vermilion Twp 2': 'PRECINCT VER TWP #2','Vermilion Twp 3': 'PRECINCT VER TWP #3','Vermilion Twp 4': 'PRECINCT VER TWP #4',\n",
    "                  'Vermilion Twp 5': 'PRECINCT VER TWP #5'}\n",
    "erie.loc[erie['Precinct_I'].isin(erie_prec_dict.keys()), \"prec_id\"] = erie[\"Precinct_I\"].map(erie_prec_dict)\n",
    "#print(\"Not sure why the plot with everything is missing the shape in the bottom left present when plot without Groton...\")\n",
    "#erie[erie[\"Precinct_I\"].str.contains(\"Groton\")].plot()\n",
    "#erie[~erie[\"Precinct_I\"].str.contains(\"Groton\")].plot()\n",
    "#erie.plot()\n",
    "#erie[erie[\"Precinct_I\"].str.contains(\"Bellevue\")].plot()\n",
    "#erie[~erie[\"Precinct_I\"].str.contains(\"Bellevue\")].plot()\n",
    "#Dissolve by prec id so Bellevue goes with Groton\n",
    "erie.loc[erie[\"Precinct_I\"]==\"Bellevue City-Annexed-Refer to Board of Elections\",\"prec_id\"] = \"PRECINCT GRO TWP\"\n",
    "erie = erie.dissolve(by=\"prec_id\").reset_index()\n",
    "#Standard\n",
    "erie[\"county\"] = \"ERIE\"\n",
    "erie[\"UNIQUE_ID\"] = erie[\"county\"]+\"-\"+erie[\"prec_id\"]\n",
    "erie = erie.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Lorain: Full match\n",
    "lorain = gp.read_file(\"./raw-from-source/boundaries/lorain_precincts_2023/full_lorain_county_06302023/full_lorain_county_06302023.shp\")\n",
    "lorain_dict_df = pd.read_csv(\"./raw-from-source/boundaries/lorain_precincts_2023/Lorain County Precinct Matching.xlsx - Lorain Co. Shapefile Precincts.csv\")\n",
    "lorain_dict = pd.Series(lorain_dict_df[\"SOS PRECINCT NAME\"].values, index = lorain_dict_df[\"PRECINCT\"]).to_dict()\n",
    "lorain_dict['LOC 2-F'] = 'PRECINCT LORAIN CITY 2-F'\n",
    "lorain_dict['LOC 8-G'] = 'PRECINCT LORAIN CITY 8-G'\n",
    "lorain_dict['LOC 8-H'] = 'PRECINCT LORAIN CITY 8-H'\n",
    "lorain_dict['NRC 1-F'] = 'PRECINCT N. RIDGEVILLE 1-F'\n",
    "lorain_dict['NRC 3-H'] = 'PRECINCT N. RIDGEVILLE 3-H'\n",
    "\n",
    "lorain[\"prec_id\"] = lorain[\"PRECINCT\"].map(lorain_dict)\n",
    "lorain = lorain.dissolve(by=\"prec_id\").reset_index()\n",
    "lorain[\"county\"] = \"LORAIN\"\n",
    "lorain[\"UNIQUE_ID\"] = lorain[\"county\"]+\"-\"+lorain[\"prec_id\"]\n",
    "lorain = lorain.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Lucas: Full match\n",
    "lucas = gp.read_file(\"./raw-from-source/boundaries/lucas_precincts_2022/full_lucas_fixed/full_lucas_fixed.shp\")\n",
    "lucas_dict_df = pd.read_csv(\"./raw-from-source/boundaries/lucas_precincts_2022/Lucas_precinctMatching.csv\")\n",
    "lucas_dict_df.loc[~lucas_dict_df[\"mismatch comment\"].isna(), \"shapefile W___P\"] = lucas_dict_df[\"mismatch W___P\"]\n",
    "lucas_dict = pd.Series(lucas_dict_df[\"SoS precinct Name\"].values, index=lucas_dict_df[\"shapefile W___P\"]).to_dict()\n",
    "lucas[\"prec_id\"] = lucas[\"W___P\"].map(lucas_dict)\n",
    "lucas.loc[lucas[\"W___P\"]==\"Mo10\", \"prec_id\"] = \"PRECINCT MONCLOVA 10\"\n",
    "lucas[\"county\"] = \"LUCAS\"\n",
    "lucas = lucas.dissolve(by=\"prec_id\").reset_index()\n",
    "lucas[\"UNIQUE_ID\"] = lucas[\"county\"]+\"-\"+lucas[\"prec_id\"]\n",
    "lucas = lucas.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Miami: Full match except precinct names are not unique - 87 prec names but 104 geometries...\n",
    "miami = gp.read_file(\"./raw-from-source/boundaries/miami_precincts_2022/All_Precincts.shp\")\n",
    "miami[\"prec_id\"] = \"PRECINCT \"+miami[\"PrecName\"].str.upper()\n",
    "#DISSOLVE BY PREC\n",
    "miami = miami.dissolve(by = \"prec_id\").reset_index()\n",
    "#MATCH PREC NAME\n",
    "miami_dict = {'BROWN-FLETCHER':'PRECINCT BROWN/FLETCHER','LAURA WEST':'PRECINCT LAURA','LOST CREEK-CASSTOWN':'PRECINCT LOSTCREEK/CASSTOWN',\n",
    "              'POTSDAM WEST':'PRECINCT POTSDAM','SPRING CREEK EAST':'PRECINCT SPRINGCREEK EAST','SPRING CREEK WEST':'PRECINCT SPRINGCREEK WEST',\n",
    "              'TROY 1A':'PRECINCT TROY 1-A','TROY 1B':'PRECINCT TROY 1-B','TROY 1C':'PRECINCT TROY 1-C','TROY 1D':'PRECINCT TROY 1-D',\n",
    "              'TROY 2A':'PRECINCT TROY 2-A','TROY 2B':'PRECINCT TROY 2-B', 'TROY 2C':'PRECINCT TROY 2-C', 'TROY 3A':'PRECINCT TROY 3-A',\n",
    "              'TROY 3B':'PRECINCT TROY 3-B', 'TROY 3C':'PRECINCT TROY 3-C', 'TROY 3D':'PRECINCT TROY 3-D', 'TROY 4A':'PRECINCT TROY 4-A',\n",
    "              'TROY 4B':'PRECINCT TROY 4-B','TROY 4C':'PRECINCT TROY 4-C','TROY 5A':'PRECINCT TROY 5-A','TROY 5B':'PRECINCT TROY 5-B',\n",
    "              'TROY 5C':'PRECINCT TROY 5-C','TROY 6A':'PRECINCT TROY 6-A','TROY 6B':'PRECINCT TROY 6-B','TROY 6C':'PRECINCT TROY 6-C',\n",
    "              'TROY 6D':'PRECINCT TROY 6-D'}\n",
    "miami.loc[miami[\"PrecName\"].str.upper().isin(miami_dict.keys()), \"prec_id\"] = miami[\"PrecName\"].str.upper().map(miami_dict)\n",
    "#STANDARD FORMATTING\n",
    "miami[\"county\"] = \"MIAMI\"\n",
    "miami[\"UNIQUE_ID\"] = miami[\"county\"]+\"-\"+miami[\"prec_id\"]\n",
    "miami = miami.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Montgomery: Full match//more geometries than there are precinct names/in election results... \n",
    "montgomery = gp.read_file(\"./raw-from-source/boundaries/montgomery_precincts_2022/precinct_2022_polygon.shp\")\n",
    "#MATCH PREC NAMES\n",
    "montgomery_dict_df = pd.read_csv(\"./raw-from-source/boundaries/montgomery_precincts_2022/prec_name_matching.csv\")\n",
    "montgomery_dict = pd.Series(montgomery_dict_df[\"ER\"].str.replace(\"'\",\"\").str.replace(\",\",\"\").str.slice(start=1).values, index=montgomery_dict_df[\"PB\"].str.replace(\"'\",\"\").str.replace(\",\",\"\").str.slice(start=1)).to_dict()\n",
    "#Set prec id\n",
    "montgomery[\"prec_id\"] = montgomery[\"VNAME\"].str.upper().str.replace(\"TOWNSHIP\", \"TWP\")\n",
    "montgomery.loc[montgomery[\"VNAME\"].isin(montgomery_dict.keys()), \"prec_id\"] = montgomery[\"prec_id\"].map(montgomery_dict)\n",
    "montgomery.loc[montgomery[\"prec_id\"]==\"JACKSON TWP_NEW LEBANON A\",\"prec_id\"] = 'JACK/NEW LEBANON-A'\n",
    "montgomery.loc[montgomery[\"prec_id\"]==\"JACKSON TWP_NEW LEBANON B\", \"prec_id\"] = 'JACK/NEW LEBANON-B'\n",
    "montgomery.loc[montgomery[\"prec_id\"]==\"PERRY TWP_NEW LEBANON\", \"prec_id\"] = 'PER/NEW LEBANON'\n",
    "#DISSOLVE BY PREC\n",
    "montgomery = montgomery.dissolve(by=\"prec_id\").reset_index()\n",
    "#STANDARD FORMATTING\n",
    "montgomery[\"county\"] = \"MONTGOMERY\"\n",
    "montgomery[\"UNIQUE_ID\"] = montgomery[\"county\"]+\"-\"+montgomery[\"prec_id\"]\n",
    "montgomery = montgomery.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Medina: full match, but more polygons than precincts in ER\n",
    "medina = gp.read_file(\"./raw-from-source/boundaries/medina_precincts_2022/precincp22.shp\")\n",
    "#MATCH PREC NAMES\n",
    "medina[\"prec_id\"] = \"PRECINCT \"+medina[\"PREC_NAME\"].str.replace(\"TWP\", \"TP\").str.replace(\"BRUNSWICK HILLS\", \"BRHILLS\").str.replace(\"HARRISVILLE TP A\",'HARRISVL TP A').str.replace(\"MEDINA CITY 3C\",\"MEDINA CITY 3-C\").str.replace(\n",
    "    \"MONTVILLE TP I\",\"MONTVILLE I\").str.replace('HARRISVILLE TWP A',\"HARRISVL TP A\")\n",
    "medina.loc[medina[\"PREC_NAME\"]==\"HOMER TP\", \"prec_id\"] = 'PRECINCT HOMER TWP'\n",
    "#DISSOLVE BY PREC\n",
    "medina = medina.dissolve(by=\"prec_id\").reset_index()\n",
    "#STANDARD FORMATTING\n",
    "medina[\"county\"] = \"MEDINA\"\n",
    "medina[\"UNIQUE_ID\"] = medina[\"county\"]+\"-\"+medina[\"prec_id\"]\n",
    "medina = medina.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "\n",
    "#Pickaway: Full match\n",
    "#Not clear initially which file in folder to use- precincts has 77 rows where as ER have 43 - must use circleville with rest of state and dissolve extra shapes\n",
    "pickaway_not_clcvle = gp.read_file(\"./raw-from-source/boundaries/pickaway_precincts_2022/Precincts_Pickaway_Co_2022.shp\").to_crs(bound20_reused_counties_gdf.crs)\n",
    "pickaway_clcvle = gp.read_file(\"./raw-from-source/boundaries/pickaway_precincts_2022/Precincts_Circlevile_2022.shp\").to_crs(bound20_reused_counties_gdf.crs)\n",
    "#Not needed for matching but good for info on shapes\n",
    "pickaway_dict_df = pd.read_csv(\"./raw-from-source/boundaries/pickaway_precincts_2022/Pickaway_precinctMatching.csv\")\n",
    "pickaway = gp.GeoDataFrame(pd.concat([pickaway_not_clcvle[[\"NAME\",\"STATE_CODE\", \"geometry\"]],pickaway_clcvle[[\"NAME\",\"STATE_CODE\",\"geometry\"]]], ignore_index=True), crs=bound20_reused_counties_gdf.crs)\n",
    "pickaway[\"prec_id\"] = pickaway[\"NAME\"]\n",
    "#print(\"Pickaway check if na name leads to holes in map or not - it does not\")\n",
    "#pickaway.loc[pickaway[\"NAME\"].isna()].plot()\n",
    "#pickaway.loc[~pickaway[\"NAME\"].isna()].plot()\n",
    "#pickaway.plot()\n",
    "#DISSOLVE BY PREC + cut out NA\n",
    "pickaway = pickaway.loc[~pickaway[\"NAME\"].isna()].dissolve(by=\"prec_id\").reset_index().to_crs(bound20_reused_counties_gdf.crs)\n",
    "pickaway[\"county\"] = \"PICKAWAY\"\n",
    "pickaway[\"UNIQUE_ID\"] = pickaway[\"county\"]+\"-\"+pickaway[\"prec_id\"]\n",
    "\n",
    "\n",
    "#Stark: Full Match\n",
    "#Andrew instructs to use json labeled Nov 2022 instead of \"Voting_Precincts zip\"\n",
    "stark = gp.read_file(\"./raw-from-source/boundaries/stark_precincts_2022/Stark County Precincts (Nov 2022).json\")\n",
    "stark[\"prec_id\"] = stark[\"NAME\"].str.replace(\"TWP\",\"\").str.replace(\"  \",\" \")\n",
    "stark.loc[stark[\"NAME\"].str.replace(\"TWP\",\"\").str.replace(\"  \",\" \").isin(['CANTON 1','CANTON 2','CANTON 3','CANTON 4','CANTON 5','CANTON 6','CANTON 7']), \"prec_id\"]= stark[\"NAME\"]\n",
    "stark.loc[stark[\"NAME\"]==\"MEYERS LAKE VILLAGE A\",\"prec_id\"]='MEYERS LAKE A'\n",
    "stark[\"county\"] = \"STARK\"\n",
    "stark[\"UNIQUE_ID\"] = stark[\"county\"] + \"-\"+stark[\"prec_id\"]\n",
    "stark = stark.dissolve(by =\"UNIQUE_ID\").reset_index()\n",
    "stark = stark.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "#Check if uniqueness/match fixed\n",
    "too_many_polygons = [brown, butler, clermont, cuyahoga, erie, miami, montgomery, medina, lorain, lucas, pickaway, stark] \n",
    "check_precid_uniqueness(too_many_polygons)\n",
    "\n",
    "\n",
    "#2020 Wood County from VEST\n",
    "wood20 = bound20rdh[bound20rdh[\"COUNTYFP20\"]==\"173\"]\n",
    "\n",
    "\n",
    "#Get 2022 new precincts drawn in DRA - Draw new precincts in DRA using shapes provided by OE from Wood County\n",
    "dra = gp.read_file(\"./dra-district-shapes/POLYGON.shp\")\n",
    "wood_new22 = dra[(dra[\"NAME\"]=='58')|(dra[\"NAME\"]=='65')]\n",
    "wood_new22 = wood_new22.to_crs(wood20.crs)\n",
    "wood_new22[\"UNIQUE_ID\"] = 0\n",
    "wood_new22[\"prec_code\"] = 0\n",
    "wood_new22.loc[wood_new22[\"NAME\"]==\"58\", \"prec_code\"] = 'AEL'\n",
    "wood_new22.loc[wood_new22[\"NAME\"]==\"65\", \"prec_code\"] = 'AEM'\n",
    "wood_new22.loc[wood_new22[\"NAME\"]==\"58\", \"UNIQUE_ID\"] = 'WOOD-PERRYSBURG J'\n",
    "wood_new22.loc[wood_new22[\"NAME\"]==\"65\", \"UNIQUE_ID\"] = 'WOOD-PERRYSBURG S'\n",
    "\n",
    "\n",
    "#Step 1: remove new wood prec area from 2020 wood prec\n",
    "wood20_min_new22 = wood20.overlay(wood_new22, how=\"symmetric_difference\")\n",
    "wood20_min_new22[\"UNIQUE_ID\"] = wood20_min_new22[\"UNIQUE_ID_1\"]\n",
    "wood20_min_new22.loc[wood20_min_new22[\"UNIQUE_ID_1\"].isna(), \"UNIQUE_ID\"]=wood20_min_new22[\"UNIQUE_ID_2\"]\n",
    "\n",
    "#Step 2: add new prec to 2020 shape\n",
    "wood22 = gp.GeoDataFrame(pd.concat([wood20_min_new22, wood_new22], ignore_index=True), crs = wood20.crs)\n",
    "\n",
    "\n",
    "wood = wood22.copy()\n",
    "\n",
    "\n",
    "wood_dict_df = pd.read_csv(\"./raw-from-source/boundaries/wood_precincts_2022/Wood_precinctMatching.csv\")\n",
    "wood_dict = pd.Series(wood_dict_df[\"SoS name\"].values, index=wood_dict_df[\"SoS 2022 code\"]).to_dict()\n",
    "wood.loc[wood[\"prec_code\"].isna(), \"prec_code\"]= wood[\"PRECINCT20\"]\n",
    "wood[\"prec_id\"] = wood[\"prec_code\"].map(wood_dict)\n",
    "wood[\"county\"] = \"WOOD\"\n",
    "wood[\"UNIQUE_ID\"] = wood[\"county\"]+\"-\"+wood[\"prec_id\"]\n",
    "#wood.loc[wood[\"prec_id\"].isna(), \"prec_id\"] = \"PERRYSBURG S\"\n",
    "wood = wood.dissolve(by=\"UNIQUE_ID\").reset_index().to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "check_precid_uniqueness([wood])\n",
    "\n",
    "\n",
    "#Portage: \n",
    "portage = gp.read_file(\"./raw-from-source/boundaries/portage_precincts_2022/precinct_2022.shp\")\n",
    "portage[\"prec_id\"] = portage[\"NAME\"]+\"-\"+portage[\"PRECINCT\"]\n",
    "portage_prec_dict_df = pd.read_csv(\"./raw-from-source/boundaries/portage_precincts_2022/Portage County Precinct Matching_2022.xlsx - Portage Co. Shapefile Precincts.csv\")\n",
    "portage_dict = pd.Series(portage_prec_dict_df[\"SoS Precinct Match\"].values, index=portage_prec_dict_df[\"NAME\"]+\"-\"+portage_prec_dict_df[\"PRECINCT\"]).to_dict()\n",
    "portage.loc[portage[\"prec_id\"].isin(portage_dict.keys()), \"prec_id\"] = portage[\"prec_id\"].map(portage_dict)\n",
    "portage.loc[portage[\"NAME\"].str.upper()=='HIRAM', \"prec_id\"] = 'PRECINCT HIRAM VILLAGE'\n",
    "portage.loc[portage[\"NAME\"].str.upper()=='MANTUA', \"prec_id\"]= 'PRECINCT MANTUA VILLAGE'\n",
    "portage.loc[portage[\"NAME\"].str.upper()=='MOGADORE', \"prec_id\"] = 'PRECINCT MOGADORE VILLAGE'\n",
    "portage.loc[portage[\"NAME\"].isna(), \"prec_id\"] = 'PRECINCT SUGAR BUSH KNOLLS'\n",
    "\n",
    "#Combine Tallmadge multipolygon with Brimfield precincts A and E\n",
    "tallmadge_multi = portage[portage[\"NAME\"]==\"Tallmadge\"]\n",
    "tallmadge_multi=tallmadge_multi.explode(index_parts=True).reset_index()\n",
    "tallmadge_multi.loc[(tallmadge_multi[\"NAME\"]==\"Tallmadge\")&(tallmadge_multi[\"level_1\"]==0), \"prec_id\"] = \"PRECINCT BRIMFIELD A\"\n",
    "tallmadge_multi.loc[(tallmadge_multi[\"NAME\"]==\"Tallmadge\")&(tallmadge_multi[\"level_1\"]==1), \"prec_id\"] = \"PRECINCT BRIMFIELD E\"\n",
    "tallmadge_multi = tallmadge_multi.to_crs(bound20_reused_counties_gdf.crs)\n",
    "#filter out what will be added back in\n",
    "portage = portage[~(portage[\"NAME\"]==\"Tallmadge\")].to_crs(bound20_reused_counties_gdf.crs)\n",
    "#combine dfs back together\n",
    "portage = gp.GeoDataFrame(pd.concat([portage, tallmadge_multi], ignore_index=True), crs=bound20_reused_counties_gdf.crs)\n",
    "\n",
    "#Combine Ravenna null with Ravenna G in accordance with email exchanges\n",
    "portage.loc[(portage[\"PRECINCT\"].isna())&(portage[\"NAME\"].str.upper().str.contains(\"RAVENNA\")), \"prec_id\"] = 'PRECINCT RAVENNA TWP G'\n",
    "#Assign ZZZ to match other 0-pop precs in the file\n",
    "portage.loc[portage[\"PRECINCT\"]==\"Arsenal\", \"prec_id\"] = \"ZZZ\"\n",
    "\n",
    "#Dissolve\n",
    "portage = portage.dissolve(by=\"prec_id\").reset_index()\n",
    "\n",
    "portage[\"county\"] = \"PORTAGE\"\n",
    "portage[\"UNIQUE_ID\"] = portage[\"county\"]+\"-\"+portage[\"prec_id\"]\n",
    "portage = portage.to_crs(bound20_reused_counties_gdf.crs)\n",
    "\n",
    "#Check if uniqueness/match fixed\n",
    "zzz_inc_county = [portage]\n",
    "print(set(er22[\"PRECNAME\"][er22[\"County\"]==\"Portage\"])-set(portage[\"prec_id\"]))\n",
    "print(set(portage[\"prec_id\"])-set(er22[\"PRECNAME\"][er22[\"County\"]==\"Portage\"]))\n",
    "\n",
    "\n",
    "#Add columns to 2020 gdf to enable join\n",
    "bound20_reused_counties_gdf[\"UNIQUE_ID_og\"] = bound20_reused_counties_gdf[\"UNIQUE_ID\"].copy()\n",
    "bound20_reused_counties_gdf[\"UNIQUE_ID\"] = bound20_reused_counties_gdf[\"UNIQUE_ID_code\"]\n",
    "bound20_reused_counties_gdf=bound20_reused_counties_gdf[bound20_reused_counties_gdf[\"UNIQUE_ID\"]!=\"PORTAGE-ZZZ\"]\n",
    "bound20_reused_counties_gdf[\"county\"] = bound20_reused_counties_gdf[\"COUNTYNM\"].str.upper()\n",
    "\n",
    "\n",
    "gdfs_2022_w_mods = [clark, columbiana, delaware, geauga, hamilton, hardin, hocking, lake, marion,mercer, muskingum, tuscarawas]+[brown, butler, clermont, cuyahoga, erie, miami, montgomery, medina, lorain, lucas, pickaway, stark]+[wood]+[portage]\n",
    "print(len(gdfs_2022_w_mods))\n",
    "#Waiting on manual mod for LORAIN\n",
    "\n",
    "wip_full_match_list = gdfs_2022_w_mods+[bound20_reused_counties_gdf]\n",
    "wip_full_match_gdf = gp.GeoDataFrame(pd.concat(wip_full_match_list, ignore_index=True), crs=bound20_reused_counties_gdf.crs)\n",
    "wip_full_match_gdf = wip_full_match_gdf[[\"UNIQUE_ID\", \"prec_id\", \"county\", \"COUNTYFP20\", \"COUNTYNM\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e86cff-7dda-4cb5-a574-4a56672e64de",
   "metadata": {},
   "source": [
    "## Combine PB and ER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c21171e-142d-4b2a-ac62-1457e29c8326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge unique_id count:  8941 \n",
      "pb_updated unique_id null count:  0\n",
      "merge shape:  (8941, 265)\n",
      "er22 unique_id count:  8933 \n",
      "er22 shape:  (8933, 260)\n",
      "\n",
      "checking statewide gdf:\n",
      "unique_id is unique\n",
      "***Statewide Totals Check***\n",
      "All contests match statewide!\n",
      "\n",
      "***Countywide Totals Check***\n",
      "All contests in all counties match!\n",
      "\n",
      "***Precinct Totals Check***\n",
      "\n",
      "There are  8933  total rows\n",
      "8933  of these rows are the same\n"
     ]
    }
   ],
   "source": [
    "er22.loc[er22[\"County\"].str.lower().isin(counties_reused22_set), \"UNIQUE_ID\"] = er22[\"County\"].str.upper() + \"-\"+er22[\"PRECCODE\"]\n",
    "er22[\"UNIQUE_ID\"] = er22[\"UNIQUE_ID\"].str.upper()\n",
    "er22[\"county\"] = er22[\"County\"].str.upper()\n",
    "\n",
    "wip_pber_merge = pd.merge(wip_full_match_gdf, er22, on = [\"UNIQUE_ID\", \"county\"], how = \"outer\", indicator=True)\n",
    "\n",
    "\n",
    "print(\"merge unique_id count: \",wip_full_match_gdf[\"UNIQUE_ID\"].nunique(),\"\\npb_updated unique_id null count: \",len(wip_full_match_gdf[\"UNIQUE_ID\"][wip_full_match_gdf[\"UNIQUE_ID\"].isna()]))\n",
    "print(\"merge shape: \",wip_pber_merge.shape)\n",
    "print(\"er22 unique_id count: \",er22[\"UNIQUE_ID\"].nunique(),\"\\ner22 shape: \",er22.shape)\n",
    "\n",
    "\n",
    "merge = wip_pber_merge.copy()\n",
    "election_cols = sorted(list(merge.columns[merge.columns.str.startswith(\"G\")]))\n",
    "merge[\"COUNTYNM\"] = merge[\"county\"]\n",
    "merge[\"COUNTYFP\"] = merge[\"county\"].str.lower().map(county_name_to_fips_dict)\n",
    "merge[\"PRECINCT\"] = merge[\"UNIQUE_ID\"].str.split(pat=\"-\").str[1]\n",
    "merge[election_cols]=merge[election_cols].fillna(0)\n",
    "assert merge[merge.columns[merge.columns.str.startswith(\"G\")]].isna().any().all()==False\n",
    "\n",
    "merge = merge[[\"UNIQUE_ID\",\"COUNTYFP\",\"COUNTYNM\",\"PRECINCT\",\"PRECCODE\",]+election_cols+[\"geometry\"]]\n",
    "\n",
    "\n",
    "#Check that no null values and that UNIQUE precinct identifier is in fact unique\n",
    "#Check that no null values and that UNIQUE precinct identifier is in fact unique\n",
    "def check_unique_id_unique(merged_gdf):\n",
    "    assert merged_gdf[\"UNIQUE_ID\"].isna().any()==False\n",
    "    assert merged_gdf[\"UNIQUE_ID\"].nunique()==merged_gdf.shape[0]\n",
    "    return \"unique_id is unique\"\n",
    "\n",
    "\n",
    "#State, County, Precinct total vote checks adapted from pdv checks: https://github.com/nonpartisan-redistricting-datahub/pdv-resources/blob/main/pdv_functions.py\n",
    "def statewide_totals_check(partner_df, partner_name, source_df, source_name, column_list):\n",
    "    \"\"\"Compares the totals of two election result dataframes at the statewide total level\n",
    "\n",
    "    Args:\n",
    "      partner_df: DataFrame of election results we are comparing against\n",
    "      source_df: DataFrame of election results we are comparing to\n",
    "      column_list: List of races that there are votes for\n",
    " \n",
    "    Returns:\n",
    "      difference list\n",
    "    \"\"\"\n",
    "    print(\"***Statewide Totals Check***\")\n",
    "    diff_races=[]\n",
    "    for race in column_list:\n",
    "        if (partner_df[race].sum()- source_df[race].sum() != 0):\n",
    "            if race not in diff_races:\n",
    "                diff_races.append(race)\n",
    "            print(race+\" has a difference of \"+str(partner_df[race].sum()-source_df[race].sum())+\" votes\")\n",
    "            print(\"\\t\"+ partner_name + \": \"+str(partner_df[race].sum())+\" votes\")\n",
    "            print(\"\\t\"+ source_name +\": \"+str(source_df[race].sum())+\" votes\")\n",
    "        #else:\n",
    "            #print(race + \" is equal\", \"\\t both dataframes \" + str(partner_df[race].sum()))\n",
    "    \n",
    "    if (len(diff_races)==0):\n",
    "        print(\"All contests match statewide!\")\n",
    "    elif (len(diff_races)>0):\n",
    "        print(\"Contests with differences: \")\n",
    "    \n",
    "    return diff_races\n",
    "\n",
    "\n",
    "def county_totals_check(partner_df, partner_name, source_df, source_name, column_list,county_col,full_print=False):\n",
    "    \"\"\"Compares the totals of two election result dataframes at the county level\n",
    "\n",
    "    Args:\n",
    "      partner_df: DataFrame of election results we are comparing against\n",
    "      partner_name: String of what to call the partner in the print statement\n",
    "      source_df: DataFrame of election results we are comparing to\n",
    "      source_name: String of what to call the source in the print statement\n",
    "      column_list: List of races that there are votes for\n",
    "      county_col: String of the column name that contains county information\n",
    "      full_print: Boolean specifying whether to print out everything, including counties w/ similarities\n",
    "\n",
    "    Returns:\n",
    "      difference list\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n***Countywide Totals Check***\")\n",
    "    diff_counties=[]\n",
    "    for race in column_list:\n",
    "        diff = partner_df.groupby([county_col]).sum()[race]-source_df.groupby([county_col]).sum()[race]\n",
    "        for val in diff[diff != 0].index.values.tolist():\n",
    "            if val not in diff_counties:\n",
    "                diff_counties.append(val)\n",
    "        if len(diff[diff != 0]!=0):   \n",
    "            print(race + \" contains differences in these counties:\")\n",
    "            for val in diff[diff != 0].index.values.tolist():\n",
    "                county_differences = diff[diff != 0]\n",
    "                print(\"\\t\"+val+\" has a difference of \"+str(county_differences[val])+\" votes\")\n",
    "                print(\"\\t\\t\"+ partner_name + \": \"+str(partner_df.groupby([county_col]).sum().loc[val,race])+\" votes\")\n",
    "                print(\"\\t\\t\"+ source_name +\": \"+str(source_df.groupby([county_col]).sum().loc[val,race])+\" votes\")\n",
    "            if (full_print):\n",
    "                for val in diff[diff == 0].index.values.tolist():\n",
    "                    county_similarities = diff[diff == 0]\n",
    "                    print(\"\\t\"+val + \": \"+ str(partner_df.groupby([county_col]).sum().loc[val,race])+\" votes\")\n",
    "        #else:\n",
    "            #print(race + \" is equal across all counties\")\n",
    "            #if (full_print):\n",
    "               # for val in diff[diff == 0].index.values.tolist():\n",
    "                 #   county_similarities = diff[diff == 0]\n",
    "                    #print(\"\\t\"+val + \": \"+ str(partner_df.groupby([county_col]).sum().loc[val,race])+\" votes\")\n",
    "    if (len(diff_counties)==0):\n",
    "        print(\"All contests in all counties match!\")\n",
    "    elif (len(diff_counties)>0):\n",
    "        print(\"Counties with differences: \")\n",
    "        \n",
    "    return diff_counties\n",
    "        \n",
    "    \n",
    "def precinct_votes_check(merged_df,column_list,vest_on_left,name_col,print_level=0):\n",
    "    \"\"\"Checks a merged dataframe with two election results at the precinct level\n",
    "\n",
    "    Args:\n",
    "      merged_df: DataFrame with one set of election results joined to another\n",
    "      column_list: List of races that there are votes for\n",
    "      vest_on_left: Boolean specifying whether VEST data is on the left side of merged_df\n",
    "      name_col: String of the column name to refer to precincts when a difference occurs\n",
    "      print_level: Integer that specifies how large the vote difference in a precinct must be to be printed\n",
    "\n",
    "    Returns:\n",
    "      list of differences\n",
    "    \"\"\"\n",
    "    print(\"\\n***Precinct Totals Check***\")\n",
    "    merged_df = merged_df.sort_values(by=[name_col],inplace=False)\n",
    "    matching_rows = 0\n",
    "    different_rows = 0\n",
    "    diff_list=[]\n",
    "    diff_values = []\n",
    "    max_diff = 0\n",
    "    for index,row in merged_df.iterrows():\n",
    "        same = True\n",
    "        for i in column_list:\n",
    "            left_data = i + \"_x\"\n",
    "            right_data = i + \"_y\"\n",
    "            \n",
    "            if ((row[left_data] is None) or (row[right_data] is None) or (np.isnan(row[right_data])or(np.isnan(row[left_data])))):\n",
    "                print(\"FIX NaN value at: \", row[name_col])\n",
    "            \n",
    "            diff = abs(row[left_data]-row[right_data])\n",
    "            if (diff>0):\n",
    "                same = False\n",
    "                diff_values.append(abs(diff))\n",
    "                if (diff>max_diff):\n",
    "                    max_diff = diff\n",
    "            if(diff>print_level):\n",
    "                if (vest_on_left):\n",
    "                    print(i, \"{:.>72}\".format(row[name_col]), \"(V)\",\"{:.>5}\".format(int(row[left_data])),\" (S){:.>5}\".format(int(row[right_data])),\"(D):{:>5}\".format(int(row[left_data]-row[right_data])))                           \n",
    "                else:\n",
    "                    print(i, \"{:.>72}\".format(row[name_col]), \"(S)\",\"{:.>5}\".format(int(row[left_data])),\" (V){:.>5}\".format(int(row[right_data])),\"(D):{:>5}\".format(int(row[left_data]-row[right_data])))\n",
    "        if(same != True):\n",
    "            different_rows +=1\n",
    "            diff_list.append(row[name_col])\n",
    "        else:\n",
    "            matching_rows +=1\n",
    "\n",
    "    print(\"\\nThere are \", len(merged_df.index),\" total rows\")\n",
    "    \n",
    "    if(len(diff_values)!=0):\n",
    "        print(matching_rows,\" of these rows are the same\")\n",
    "        print(\"\\nAll precincts containing differences:\")\n",
    "        print(\"The average difference is: \", str(sum(diff_values)/len(diff_values)))\n",
    "        print(\"\\nThe max difference between any one shared column in a row is: \", max_diff)\n",
    "        count_big_diff = len([i for i in diff_values if i > 10])\n",
    "        print(\"There are \", str(count_big_diff), \"precinct results with a difference greater than 10\")\n",
    "    else:\n",
    "        print(matching_rows,\" of these rows are the same\")\n",
    "    \n",
    "    diff_list.sort()\n",
    "    \n",
    "    return diff_list\n",
    "\n",
    "\n",
    "def run_all_checks(partner_df, partner_name, source_df, source_name, column_list,county_col,full_print=False, prec_check=True):\n",
    "    print(check_unique_id_unique(source_df))\n",
    "    #Running inner join because of expected nan value for ZZZ precincts\n",
    "    merged_df = pd.merge(source_df, partner_df, on = [\"UNIQUE_ID\"], how = \"inner\", indicator=True)\n",
    "    vest_on_left = False\n",
    "    name_col = \"UNIQUE_ID\"\n",
    "    #All matches statewide and county levels\n",
    "    statewide_totals_check(partner_df, partner_name, source_df, source_name, column_list)\n",
    "    county_totals_check(partner_df, partner_name, source_df, source_name, column_list,county_col,full_print=False)\n",
    "    if prec_check ==True:\n",
    "        precinct_votes_check(merged_df,column_list,vest_on_left,name_col,print_level=0)\n",
    "    \n",
    "    \n",
    "partner_df = er22.copy()\n",
    "partner_df[\"COUNTYNM\"] = partner_df[\"County\"].str.upper()\n",
    "partner_name = \"original ER 22\"\n",
    "source_df = merge.copy()\n",
    "source_name = \"PBER 22\"\n",
    "county_col = \"COUNTYNM\"\n",
    "#---\n",
    "print(\"\\nchecking statewide gdf:\")\n",
    "run_all_checks(partner_df, partner_name, source_df, \"merge gdf\", list(source_df.columns[source_df.columns.str.startswith(\"G\")]),county_col,full_print=False, prec_check=True)\n",
    "#-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147b754-c69c-4aec-8e3e-11dd0be99a6d",
   "metadata": {},
   "source": [
    "## Split precinct boundaries by district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ccf171-3131-4e84-ab09-3021392fb7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in district boundary files\n",
    "cong_shp = gp.read_file(\"./raw-from-source/2022_districts/oh_cong_adopted_2022/oh_cong_adopted_2022.shp\").to_crs(merge.crs)\n",
    "cong_shp[\"CONG_DIST_2\"] = cong_shp[\"CONG_DIST\"].copy()\n",
    "cong_shp = cong_shp.drop(\"CONG_DIST\", axis=1)\n",
    "sl_shp = gp.read_file(\"./raw-from-source/2022_districts/oh_sldl_adopted_2022/FEB_24_2022_HD_SHP.shp\").to_crs(merge.crs)\n",
    "su_shp = gp.read_file(\"./raw-from-source/2022_districts/oh_sldu_adopted_2022/FEB_24_2022_SD_SHP.shp\").to_crs(merge.crs)\n",
    "\n",
    "\n",
    "#Create gdfs with appropriate elections\n",
    "merge_for_splits = merge.copy()\n",
    "merge_cong = merge_for_splits[['UNIQUE_ID', 'PRECINCT', 'PRECCODE', 'COUNTYNM','COUNTYFP']+list(merge_for_splits.columns[merge_for_splits.columns.str.startswith(\"GCON\")])+['geometry']].reset_index()\n",
    "merge_sl = merge_for_splits[['UNIQUE_ID', 'PRECINCT', 'PRECCODE', 'COUNTYNM','COUNTYFP']+list(merge_for_splits.columns[merge_for_splits.columns.str.startswith(\"GSL\")])+['geometry']].reset_index()\n",
    "merge_su = merge_for_splits[['UNIQUE_ID', 'PRECINCT', 'PRECCODE', 'COUNTYNM','COUNTYFP']+list(merge_for_splits.columns[merge_for_splits.columns.str.startswith(\"GSU\")])+['geometry']].reset_index()\n",
    "assert cong_shp.crs == sl_shp.crs == su_shp.crs == merge.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c4cd32-2f3a-497f-9068-ef6ac4b64128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to run fxn:  0.5917090000000016\n",
      "time to run fxn:  0.8777309999999972\n",
      "time to run fxn:  0.6066339999999997\n"
     ]
    }
   ],
   "source": [
    "#Find splits\n",
    "def get_contest_dist_dict(df, contest):\n",
    "    if contest ==\"GCON\":\n",
    "        col_to_dist_dict = pd.Series(df.columns[df.columns.str.startswith(contest)].str.slice(start=4,stop=6).values, index=df.columns[df.columns.str.startswith(contest)]).to_dict()\n",
    "    elif contest ==\"GSL\":\n",
    "        col_to_dist_dict = pd.Series(df.columns[df.columns.str.startswith(contest)].str.slice(start=3,stop=6).values, index=df.columns[df.columns.str.startswith(contest)]).to_dict()\n",
    "    elif contest ==\"GSU\":\n",
    "        col_to_dist_dict = pd.Series(df.columns[df.columns.str.startswith(contest)].str.slice(start=3,stop=5).values, index=df.columns[df.columns.str.startswith(contest)]).to_dict()\n",
    "    \n",
    "    return col_to_dist_dict\n",
    "\n",
    "\n",
    "def get_prec_to_dist_list_dict(df, contest):\n",
    "    col_dist_dict = get_contest_dist_dict(df, contest)\n",
    "    df[\"DIST\"] = 0\n",
    "    #Instead of iterating over columns, iterate over districts?\n",
    "    for col in df.columns[df.columns.str.startswith(contest)]:\n",
    "        df.loc[(df[col]>0)&(df[\"DIST\"]==0), \"DIST\"] = col_dist_dict.get(col)\n",
    "\n",
    "        df.loc[(df[col]>0)&(df[\"DIST\"]!=0), \"DIST\"] = df[\"DIST\"][(df[col]>0)]+\", \"+col_dist_dict.get(col)\n",
    "    \n",
    "    dist_list_dict = pd.Series(df[\"DIST\"].values, index=df[\"UNIQUE_ID\"])\n",
    "    \n",
    "    return dist_list_dict\n",
    "\n",
    "\n",
    "def get_df_with_split_prec_indicator(df, contest):\n",
    "    t = time.process_time()\n",
    "    \n",
    "    df[\"DIST\"] = df[\"UNIQUE_ID\"].map(get_prec_to_dist_list_dict(df, contest))\n",
    "    \n",
    "    df[\"DIST_index\"]=0\n",
    "    df[\"DIST_len_assignment\"]=0\n",
    "    index_to_set_dict = {}\n",
    "    index_to_len_dict = {}\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        df[\"DIST_index\"].loc[i] = i\n",
    "        if df.loc[i,\"DIST\"]==0:\n",
    "            district_list = [0]\n",
    "            index_to_len_dict[i] = 0\n",
    "        else:\n",
    "            district_list = list(set(df.loc[i,\"DIST\"].split(\", \")))\n",
    "            index_to_len_dict[i] = len(district_list)\n",
    "        index_to_set_dict[i] = district_list\n",
    "        \n",
    "        if len(district_list)==1:\n",
    "            district_item = district_list[0]\n",
    "            index_to_set_dict[i] = district_item\n",
    "    df[\"DIST_set\"] = df[\"DIST_index\"].map(index_to_set_dict)\n",
    "   \n",
    "    df[\"DIST_len_assignment\"] = df[\"DIST_index\"].map(index_to_len_dict)\n",
    "    prec_to_dist_dict = pd.Series(df[\"DIST_set\"].values, index = df[\"UNIQUE_ID\"])\n",
    "    \n",
    "    elapsed_time = time.process_time() - t\n",
    "    print(\"time to run fxn: \", elapsed_time)\n",
    "    \n",
    "    return df, prec_to_dist_dict\n",
    "\n",
    "\n",
    "#Grab splits\n",
    "cong_pber = merge_cong.copy()\n",
    "cong_pber_splits = get_df_with_split_prec_indicator(cong_pber, \"GCON\")\n",
    "cong_pber_splits_gdf = cong_pber_splits[0]\n",
    "cong_pber_splits_gdf[\"CONG_DIST\"] = 0\n",
    "cong_pber_splits_gdf[\"CONG_DIST\"] = cong_pber_splits_gdf[\"UNIQUE_ID\"].map(cong_pber_splits[1])\n",
    "cong_splits_dict = pd.Series(cong_pber_splits_gdf[\"CONG_DIST\"][cong_pber_splits_gdf[\"DIST_len_assignment\"]>1].values, index = cong_pber_splits_gdf[\"UNIQUE_ID\"][cong_pber_splits_gdf[\"DIST_len_assignment\"]>1]).to_dict()\n",
    "\n",
    "\n",
    "sl_pber_splits = get_df_with_split_prec_indicator(merge_sl, \"GSL\")\n",
    "sl_pber_splits_gdf = sl_pber_splits[0]\n",
    "sl_pber_splits_gdf[\"SL_DIST\"] = 0\n",
    "sl_pber_splits_gdf[\"SL_DIST\"] = sl_pber_splits_gdf[\"UNIQUE_ID\"].map(sl_pber_splits[1])\n",
    "sl_splits_dict = pd.Series(sl_pber_splits_gdf[\"SL_DIST\"][sl_pber_splits_gdf[\"DIST_len_assignment\"]>1].values, index = sl_pber_splits_gdf[\"UNIQUE_ID\"][sl_pber_splits_gdf[\"DIST_len_assignment\"]>1]).to_dict()\n",
    "\n",
    "\n",
    "su_pber_splits = get_df_with_split_prec_indicator(merge_su, \"GSU\")\n",
    "su_pber_splits_gdf = su_pber_splits[0]\n",
    "su_pber_splits_gdf[\"SU_DIST\"] = 0\n",
    "su_pber_splits_gdf[\"SU_DIST\"] = su_pber_splits_gdf[\"UNIQUE_ID\"].map(su_pber_splits[1])\n",
    "su_splits_dict = pd.Series(su_pber_splits_gdf[\"SU_DIST\"][su_pber_splits_gdf[\"DIST_len_assignment\"]>1].values, index = su_pber_splits_gdf[\"UNIQUE_ID\"][su_pber_splits_gdf[\"DIST_len_assignment\"]>1]).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b70c6d-46b0-465b-83d5-9daaa3f8ff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to run fxn:  0.18985299999999938\n",
      "time to run fxn:  0.870508000000001\n",
      "time to run fxn:  0.1820589999999953\n"
     ]
    }
   ],
   "source": [
    "#Create gdfs with splits\n",
    "#generates dataframes with the \"lost votes\" from splits\n",
    "def generate_differences_df(df_compare_against, df_compare_to, unique_ID_col, races_list, drop_empty = False):\n",
    "    \n",
    "    df_compare_against = df_compare_against[[unique_ID_col]+races_list]\n",
    "    df_compare_to = df_compare_to[[unique_ID_col]+races_list]\n",
    " \n",
    "    grouped_compare_against = df_compare_against.groupby(unique_ID_col).sum()\n",
    "    grouped_compare_to = df_compare_to.groupby(unique_ID_col).sum()\n",
    "    \n",
    "    grouped_compare_against.reset_index(inplace = True, drop = False)\n",
    "    grouped_compare_to.reset_index(inplace = True, drop = False)\n",
    "    diffs = grouped_compare_against.set_index(unique_ID_col).subtract(grouped_compare_to.set_index(unique_ID_col))\n",
    "    \n",
    "    diffs[\"Tot_Votes\"] = diffs[races_list].sum(axis=1)\n",
    "    \n",
    "    if drop_empty:\n",
    "        diffs = diffs.loc[~(diffs==0).all(axis=1)]\n",
    "        diffs = diffs.loc[:, (diffs != 0).any(axis=0)]\n",
    "    return diffs\n",
    "\n",
    "\n",
    "def district_splits_comb(level, splits_dict, elections_gdf, district_gdf, unique_ID_col, district_gdf_ID, races_list, elections_gdf_dist_ID, fill_level = 2):\n",
    "    '''\n",
    "    Function to split precincts across districts that splits a precinct across the entire district map.\n",
    "    Previous iterations of this code only split precincts by the districts in which votes were recorded.\n",
    "    In some instances, that led to holes in the map, due to districts where no votes were recorded in a precinct, but where an intersection occurred.\n",
    "    '''\n",
    "    t = time.process_time()\n",
    "    # Intersect the elections gdf with the district gdf\n",
    "    need_splits = elections_gdf[elections_gdf[unique_ID_col].isin(list(splits_dict.keys()))]\n",
    "    others = elections_gdf[~elections_gdf[unique_ID_col].isin(list(splits_dict.keys()))]\n",
    "    \n",
    "    pre_splits_copy = need_splits.copy(deep = True)\n",
    "    \n",
    "    test_join = gp.overlay(need_splits, district_gdf, how = \"intersection\")\n",
    "    \n",
    "    # Assign a district column, using the district shapefile\n",
    "    test_join[elections_gdf_dist_ID] = test_join[district_gdf_ID]\n",
    "    \n",
    "    # Filter the intersection down to the precinct, district pairs we need\n",
    "    clean_votes = test_join.copy(deep = True)\n",
    "    \n",
    "    clean_votes[unique_ID_col+\"_new\"] = clean_votes[unique_ID_col]\n",
    "    \n",
    "    # Remove the others and hold on to these to be merged later\n",
    "    for index, row in clean_votes.iterrows():\n",
    "        clean_votes.at[index, unique_ID_col+\"_new\"] = row[unique_ID_col]+\"-(\"+level + \"-\" + row[district_gdf_ID].zfill(fill_level) + \")\" \n",
    "        for column in test_join:\n",
    "            if column in races_list and row[elections_gdf_dist_ID].zfill(fill_level) not in column:\n",
    "                clean_votes.at[index, column] = 0 \n",
    "        \n",
    "    lost_votes_df = generate_differences_df(pre_splits_copy, clean_votes, unique_ID_col, races_list, True)\n",
    "    \n",
    "    clean_votes.drop(unique_ID_col, axis = 1, inplace = True)\n",
    "    clean_votes.rename(columns = {unique_ID_col+\"_new\":unique_ID_col}, inplace = True)\n",
    "    clean_votes = clean_votes[list(others.columns)]\n",
    "        \n",
    "    elections_gdf = gp.GeoDataFrame(pd.concat([clean_votes, others]), crs = elections_gdf.crs)\n",
    "    elections_gdf.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    elapsed_time = time.process_time() - t\n",
    "    print(\"time to run fxn: \", elapsed_time)\n",
    "    \n",
    "    return elections_gdf, lost_votes_df\n",
    "\n",
    "\n",
    "\n",
    "def clean_na_dist_assignments(elections_gdf, district_gdf, unique_ID_col, elections_gdf_dist_ID, district_gdf_ID):\n",
    "    t = time.process_time()\n",
    "    if elections_gdf[elections_gdf[elections_gdf_dist_ID]==0].shape[0]==0:\n",
    "        return elections_gdf\n",
    "    \n",
    "    original_crs = elections_gdf.crs\n",
    "    elections_gdf = elections_gdf.to_crs(3857)\n",
    "    \n",
    "    district_gdf = district_gdf.to_crs(3857)\n",
    "    \n",
    "    dist_clean = gp.overlay(elections_gdf[elections_gdf[elections_gdf_dist_ID]==0], district_gdf, how = \"intersection\")\n",
    "    dist_clean['area'] = dist_clean.area\n",
    "    na_assignment_dict = {}\n",
    "    for val in dist_clean[unique_ID_col].unique():\n",
    "        assignment = dist_clean.loc[dist_clean[unique_ID_col] == val].nlargest(1, 'area')[district_gdf_ID].values[0]\n",
    "        na_assignment_dict[val] = assignment\n",
    "    elections_gdf[elections_gdf_dist_ID] = elections_gdf[unique_ID_col].map(na_assignment_dict).fillna(elections_gdf[elections_gdf_dist_ID])    \n",
    "    elections_gdf = elections_gdf.to_crs(original_crs)\n",
    "   \n",
    "    elapsed_time = time.process_time() - t\n",
    "    print(\"time to run fxn: \", elapsed_time)\n",
    "    \n",
    "    return elections_gdf\n",
    "\n",
    "\n",
    "#Create gdfs with split geometries\n",
    "cong_splits = district_splits_comb(\"CON\", cong_splits_dict, cong_pber_splits_gdf, cong_shp, \"UNIQUE_ID\", \"CONG_DIST_2\", list(merge.columns[merge.columns.str.contains(\"GCON\")]), \"CONG_DIST\", fill_level = 2)\n",
    "cong_elections_gdf = cong_splits[0]\n",
    "cong_lost_votes = cong_splits[1]\n",
    "\n",
    "sl_dist_splits = district_splits_comb(\"SL\", sl_splits_dict, sl_pber_splits_gdf, sl_shp, \"UNIQUE_ID\", \"DISTRICT\", list(merge.columns[merge.columns.str.contains(\"GSL\")]), \"SL_DIST\", fill_level = 3)\n",
    "sl_elections_gdf = sl_dist_splits[0]\n",
    "sl_lost_votes_gdf = sl_dist_splits[1]\n",
    "\n",
    "#Not all SLDU districts held races, so it makes sense that some did not get a district assignment\n",
    "su_dist_splits = district_splits_comb(\"SU\", su_splits_dict, su_pber_splits_gdf, su_shp, \"UNIQUE_ID\", \"DISTRICT\", list(merge.columns[merge.columns.str.contains(\"GSU\")]), \"SU_DIST\", fill_level = 2)\n",
    "su_elections_gdf = su_dist_splits[0]\n",
    "su_lost_votes_gdf = su_dist_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b599abd8-7a82-4e21-99b3-00b54b5c4b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to run fxn:  5.059994000000003\n",
      "time to run fxn:  5.480855000000005\n",
      "time to run fxn:  17.569657999999997\n",
      "\n",
      "checking CONG w splits gdf:\n",
      "unique_id is unique\n",
      "***Statewide Totals Check***\n",
      "All contests match statewide!\n",
      "\n",
      "***Countywide Totals Check***\n",
      "All contests in all counties match!\n",
      "\n",
      "***Precinct Totals Check***\n",
      "\n",
      "There are  8898  total rows\n",
      "8898  of these rows are the same\n",
      "\n",
      "checking SLDL w splits gdf:   - expect mismatch for dist 55 contests as identified above\n",
      "unique_id is unique\n",
      "***Statewide Totals Check***\n",
      "GSL055DZOR has a difference of 15.0 votes\n",
      "\toriginal ER 22: 13167 votes\n",
      "\tsl split: 13152.0 votes\n",
      "GSL055RLIP has a difference of 58.0 votes\n",
      "\toriginal ER 22: 39170 votes\n",
      "\tsl split: 39112.0 votes\n",
      "Contests with differences: \n",
      "\n",
      "***Countywide Totals Check***\n",
      "GSL055DZOR contains differences in these counties:\n",
      "\tWARREN has a difference of 15.0 votes\n",
      "\t\toriginal ER 22: 13167 votes\n",
      "\t\tsl split: 13152.0 votes\n",
      "GSL055RLIP contains differences in these counties:\n",
      "\tWARREN has a difference of 58.0 votes\n",
      "\t\toriginal ER 22: 39170 votes\n",
      "\t\tsl split: 39112.0 votes\n",
      "Counties with differences: \n",
      "\n",
      "***Precinct Totals Check***\n",
      "\n",
      "There are  8842  total rows\n",
      "8842  of these rows are the same\n",
      "\n",
      "checking SLDU w splits gdf\n",
      "unique_id is unique\n",
      "***Statewide Totals Check***\n",
      "All contests match statewide!\n",
      "\n",
      "***Countywide Totals Check***\n",
      "All contests in all counties match!\n",
      "\n",
      "***Precinct Totals Check***\n",
      "\n",
      "There are  8911  total rows\n",
      "8911  of these rows are the same\n"
     ]
    }
   ],
   "source": [
    "#Note: SL lost votes not empty - 55 and 56 are assigned the same precinct, therefore was not split... Solution is to provide a file with no splits as well.\n",
    "\n",
    "\n",
    "#Clean NA DIST Assignments - because CONG and SU dist all assigned, don't need cleaned, just returns same gdf\n",
    "cleaned_cong_gdf = clean_na_dist_assignments(cong_elections_gdf, cong_shp, \"UNIQUE_ID\", \"CONG_DIST\", \"CONG_DIST_2\")\n",
    "cleaned_sl_gdf = clean_na_dist_assignments(sl_elections_gdf, sl_shp, \"UNIQUE_ID\", \"SL_DIST\", \"DISTRICT\")\n",
    "cleaned_su_gdf = clean_na_dist_assignments(su_elections_gdf, su_shp, \"UNIQUE_ID\", \"SU_DIST\", \"DISTRICT\")\n",
    "\n",
    "\n",
    "#Check results for split files\n",
    "print(\"\\nchecking CONG w splits gdf:\")\n",
    "run_all_checks(partner_df, partner_name, cleaned_cong_gdf, \"cong split\", list(cleaned_cong_gdf.columns[cleaned_cong_gdf.columns.str.startswith(\"G\")]),county_col,full_print=False, prec_check=True)\n",
    "print(\"\\nchecking SLDL w splits gdf:   - expect mismatch for dist 55 contests as identified above\")\n",
    "run_all_checks(partner_df, partner_name, cleaned_sl_gdf, \"sl split\", list(cleaned_sl_gdf.columns[cleaned_sl_gdf.columns.str.startswith(\"G\")]),county_col,full_print=False, prec_check=True)\n",
    "print(\"\\nchecking SLDU w splits gdf\")\n",
    "run_all_checks(partner_df, partner_name, cleaned_su_gdf, \"su split\", list(cleaned_su_gdf.columns[cleaned_su_gdf.columns.str.startswith(\"G\")]),county_col,full_print=False, prec_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8849bb6c-f852-45e5-854b-b7802ca94b68",
   "metadata": {},
   "source": [
    "## Finalize formatting for PBER GDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e21ea84-98db-428f-bf01-327b1fdbf2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unsplit file, all elections\n",
    "merge_all = merge[['UNIQUE_ID', 'PRECINCT', 'PRECCODE', 'COUNTYNM','COUNTYFP']+list(merge.columns[merge.columns.str.startswith(\"G\")])+['geometry']].reset_index()\n",
    "merge_all[\"VTDST22\"] = merge_all[\"COUNTYFP\"]+merge_all[\"PRECCODE\"]\n",
    "all_gdf = merge_all[['UNIQUE_ID', \"VTDST22\",'COUNTYFP', 'COUNTYNM', 'PRECINCT', 'PRECCODE']+sorted(merge_all.columns[merge_all.columns.str.startswith(\"G\")])+[\"geometry\"]]\n",
    "\n",
    "\n",
    "#Unsplit file with only statewide results\n",
    "merge_st = merge[list(merge.columns[~(merge.columns.str.startswith(\"GCON\"))&~(merge.columns.str.startswith(\"GSL\"))&~(merge.columns.str.startswith(\"GSU\"))])].reset_index(drop=True)\n",
    "merge_st[\"VTDST22\"] = merge_st[\"COUNTYFP\"]+merge_st[\"PRECCODE\"]\n",
    "st_gdf = merge_st[['UNIQUE_ID', \"VTDST22\",'COUNTYFP', 'COUNTYNM', 'PRECINCT', 'PRECCODE']+sorted(merge_st.columns[merge_st.columns.str.startswith(\"G\")])+[\"geometry\"]]\n",
    "\n",
    "\n",
    "#Cong splits\n",
    "cleaned_cong_gdf[\"VTDST22\"] = cleaned_cong_gdf[\"COUNTYFP\"]+cleaned_cong_gdf[\"PRECCODE\"]\n",
    "cong_gdf = cleaned_cong_gdf[['UNIQUE_ID', \"VTDST22\",'CONG_DIST','COUNTYFP', 'COUNTYNM', 'PRECINCT', 'PRECCODE']+sorted(cleaned_cong_gdf.columns[cleaned_cong_gdf.columns.str.startswith(\"G\")])+[\"geometry\"]]\n",
    "\n",
    "\n",
    "#SLDU splits\n",
    "cleaned_su_gdf[\"SLDU_DIST\"] = cleaned_su_gdf[\"SU_DIST\"]\n",
    "cleaned_su_gdf[\"VTDST22\"] = cleaned_su_gdf[\"COUNTYFP\"]+cleaned_su_gdf[\"PRECCODE\"]\n",
    "su_gdf = cleaned_su_gdf[['UNIQUE_ID', \"VTDST22\",'SLDU_DIST', 'COUNTYFP', 'COUNTYNM', 'PRECINCT', 'PRECCODE']+sorted(cleaned_su_gdf.columns[cleaned_su_gdf.columns.str.startswith(\"G\")])+[\"geometry\"]]\n",
    "\n",
    "\n",
    "#SLDL splits\n",
    "cleaned_sl_gdf[\"SLDL_DIST\"] = cleaned_sl_gdf[\"SL_DIST\"].str.zfill(3)\n",
    "cleaned_sl_gdf[\"VTDST22\"] = cleaned_sl_gdf[\"COUNTYFP\"]+cleaned_sl_gdf[\"PRECCODE\"]\n",
    "sl_gdf = cleaned_sl_gdf[['UNIQUE_ID', \"VTDST22\", 'SLDL_DIST', 'COUNTYFP', 'COUNTYNM', 'PRECINCT', 'PRECCODE']+sorted(cleaned_sl_gdf.columns[cleaned_sl_gdf.columns.str.startswith(\"G\")])+[\"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e1e5ba7-b476-416b-b551-48fb2e45555e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8977, 38)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2404f70-9529-4c2c-80f3-460618333e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8970, 37)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "su_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deca3045-a1ae-4e20-90ba-530250fb0a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9069, 179)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516b5526-36ce-4695-9545-b6ce8e26015a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIRMED: Actual cong votes match CONG_DIST assignment\n",
      "CONFIRMED: Actual sl votes match SLDL_DIST assignment\n",
      "CONFIRMED: Actual su votes match SLDU_DIST assignment\n"
     ]
    }
   ],
   "source": [
    "level_race_name_dict = {\"CONG_DIST\":\"CON\",\n",
    "    \"SLDL_DIST\":\"SL\",\n",
    "    \"SLDU_DIST\":\"SU\"}\n",
    "\n",
    "\n",
    "def district_assignment_errors(election_results_df, level):\n",
    "    '''\n",
    "    This function will check whether the votes in a precinct match the district assignment\n",
    "    Note: As written, this will only work for fully numeric district assignments\n",
    "    '''\n",
    "    \n",
    "    # This boolean is used to help clean up the print statements\n",
    "    any_error = False\n",
    "    # Convert from the name of the district assignment column, to how it is referred to in column names\n",
    "    finding = level_race_name_dict[level]\n",
    "    # Iterate over the dataframe row-by-row\n",
    "    for index,row in election_results_df.iterrows():\n",
    "        # Get the district assignment for that row\n",
    "        district_assignment = row[level]\n",
    "        # Iterate over every row column by column\n",
    "        for val in row.index:\n",
    "            # If the name for the type of district is in the column name and there are non-zero votes\n",
    "            if finding in val and row[val] != 0 and val!= level and val not in [\"SLDU_DIST\",\"SLDL_DIST\",\"CONG_DIST\", \"SCONG_DIST\"]:\n",
    "                # Grab the numbers for district assignments\n",
    "                regex_string = finding+'\\d*'\n",
    "                # The len(finding) part is needed here as sometimes there is more than one digit to the district\n",
    "                col_district = re.findall(regex_string, val)[0][len(finding):]\n",
    "                # Makes sure that \"CON\", \"SL\", or \"SU\" wasn't found in a name (more than 5 characters into the col name)\n",
    "                if (val.find(finding) < 5):\n",
    "                    # If the district number in the column name doesn't equal the district name in the assignment column\n",
    "                    if (col_district != district_assignment.zfill(2)):\n",
    "                        print(val.find(finding))\n",
    "                        print(re.findall(regex_string, val))\n",
    "                        if not(any_error):\n",
    "                            print(\"***ERROR SPOTTED***\")\n",
    "                        print(\"District Assignment: \", district_assignment)\n",
    "                        print(\"Value\" , col_district)\n",
    "                        print(\"Column\", val)\n",
    "                        print(\"Number of votes\", row[val])\n",
    "                        print(row[\"UNIQUE_ID\"])\n",
    "                        print(\" \")\n",
    "                        any_error = True\n",
    "    return any_error\n",
    "\n",
    "\n",
    "assert(not(district_assignment_errors(cong_gdf, \"CONG_DIST\"))), \"Bad CONG_DIST assignment\"\n",
    "print(\"CONFIRMED: Actual cong votes match CONG_DIST assignment\")\n",
    "\n",
    "assert(not(district_assignment_errors(sl_gdf, \"SLDL_DIST\"))), \"Bad SLDL_DIST assignment\"\n",
    "print(\"CONFIRMED: Actual sl votes match SLDL_DIST assignment\")\n",
    "\n",
    "assert(not(district_assignment_errors(su_gdf, \"SLDU_DIST\"))), \"Bad SLDU_DIST assignment\"\n",
    "print(\"CONFIRMED: Actual su votes match SLDU_DIST assignment\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fdd8786-5f2d-4f07-8fbb-ffc6af1d079a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-12-a17de20832ec>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-a17de20832ec>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b5936c-9b90-48e9-ac47-252542921ee0",
   "metadata": {},
   "source": [
    "## Export files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cce4e0b0-0804-47b8-a63a-ed0308dd2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gdf.to_file(\"./oh_2022_gen_prec_shp_wip/oh_2022_gen_prec_no_splits.shp\")\n",
    "st_gdf.to_file(\"./oh_2022_gen_prec_shp_wip/oh_2022_gen_prec_st.shp\")\n",
    "cong_gdf.to_file(\"./oh_2022_gen_prec_shp_wip/oh_2022_gen_prec_cong.shp\")\n",
    "su_gdf.to_file(\"./oh_2022_gen_prec_shp_wip/oh_2022_gen_prec_sldu.shp\")\n",
    "sl_gdf.to_file(\"./oh_2022_gen_prec_shp_wip/oh_2022_gen_prec_sldl.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788feb6-a7f4-4b83-a683-66915c15ce5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notes on what comes next for these files\n",
    "- Load in to separate notebook to clean, re-export and disaggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9558d03-dac6-4881-bbb7-7aff2988a517",
   "metadata": {},
   "source": [
    "# Match vote/dist assignment fxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f4d0e-6af8-4954-871e-9c77d30f84bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "level_race_name_dict = {\"CONG_DIST\":\"CON\",\n",
    "    \"SCONG_DIST\":\"CON\",\n",
    "    \"SLDL_DIST\":\"SL\",\n",
    "    \"SLDU_DIST\":\"SU\"}\n",
    "\n",
    "\n",
    "def district_assignment_errors(election_results_df, level):\n",
    "    '''\n",
    "    This function will check whether the votes in a precinct match the district assignment\n",
    "    Note: As written, this will only work for fully numeric district assignments\n",
    "    '''\n",
    "    \n",
    "    # This boolean is used to help clean up the print statements\n",
    "    any_error = False\n",
    "    # Convert from the name of the district assignment column, to how it is referred to in column names\n",
    "    finding = level_race_name_dict[level]\n",
    "    # Iterate over the dataframe row-by-row\n",
    "    for index,row in election_results_df.iterrows():\n",
    "        # Get the district assignment for that row\n",
    "        district_assignment = row[level]\n",
    "        # Iterate over every row column by column\n",
    "        for val in row.index:\n",
    "            # If the name for the type of district is in the column name and there are non-zero votes\n",
    "            if finding in val and row[val] != 0 and val!= level and val not in [\"SLDU_DIST\",\"SLDL_DIST\",\"CONG_DIST\", \"SCONG_DIST\"]:\n",
    "                # Grab the numbers for district assignments\n",
    "                regex_string = finding+'\\d*'\n",
    "                # The len(finding) part is needed here as sometimes there is more than one digit to the district\n",
    "                col_district = re.findall(regex_string, val)[0][len(finding):]\n",
    "                # Makes sure that \"CON\", \"SL\", or \"SU\" wasn't found in a name (more than 5 characters into the col name)\n",
    "                if (val.find(finding) < 5):\n",
    "                    # If the district number in the column name doesn't equal the district name in the assignment column\n",
    "                    if (col_district != district_assignment.zfill(2)):\n",
    "                        print(val.find(finding))\n",
    "                        print(re.findall(regex_string, val))\n",
    "                        if not(any_error):\n",
    "                            print(\"***ERROR SPOTTED***\")\n",
    "                        print(\"District Assignment: \", district_assignment)\n",
    "                        print(\"Value\" , col_district)\n",
    "                        print(\"Column\", val)\n",
    "                        print(\"Number of votes\", row[val])\n",
    "                        print(row[\"UNIQUE_ID\"])\n",
    "                        print(\" \")\n",
    "                        any_error = True\n",
    "    return any_error\n",
    "\n",
    "\n",
    "assert(not(district_assignment_errors(su_gdf, \"SLDU_DIST\"))), \"Bad SLDU_DIST assignment\"\n",
    "print(\"CONFIRMED: Actual su votes match SLDU_DIST assignment\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99196377-9be7-4ea5-8330-0a0b733814a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(not(district_assignment_errors(cong_gdf, \"CONG_DIST\"))), \"Bad CONG_DIST assignment\"\n",
    "print(\"CONFIRMED: Actual cong votes match CONG assignment\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77724bac-88aa-40d6-8c43-c902941107a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(not(district_assignment_errors(sl_gdf, \"SLDL_DIST\"))), \"Bad SLDL_DIST assignment\"\n",
    "print(\"CONFIRMED: Actual sldl votes match SLDL assignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d2f99-280e-4d31-981e-2b96b789b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "su_pber_splits_gdf[list(su_pber_splits_gdf.columns[su_pber_splits_gdf.columns.str.startswith(\"GSU03\")|su_pber_splits_gdf.columns.str.startswith(\"GSU25\")])][su_pber_splits_gdf[\"UNIQUE_ID\"]==\"FRANKLIN-BAV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5289750-928f-4da2-8511-f7eaddf17efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_su[list(merge_su.columns[merge_su.columns.str.startswith(\"GSU03\")|merge_su.columns.str.startswith(\"GSU25\")])][merge_su[\"UNIQUE_ID\"]==\"FRANKLIN-BAV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2019b-3c07-415c-8b70-8a8ec2a4c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "su_gdf[(su_gdf[\"SLDU_DIST\"]==\"3\")&(su_gdf[\"GSU25RWYS\"]==6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbbac02-14d9-47db-8d98-77954660c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_su_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99574444-ca5b-4de5-a508-33f51d4d7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_su_gdf[cleaned_su_gdf[\"UNIQUE_ID\"]==\"FRANKLIN-BAV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5214b-0cdc-4f56-b8f4-07db9481a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|cleaned_su_gdf.columns.str.startswith(\"GSU25\")\n",
    "list(cleaned_su_gdf.columns[cleaned_su_gdf.columns.str.startswith(\"GSU03\")|cleaned_su_gdf.columns.str.startswith(\"GSU03\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75575ce-c29a-4a54-af8a-1b72c0dff668",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_su_gdf[list(cleaned_su_gdf.columns[cleaned_su_gdf.columns.str.startswith(\"GSU03\")|cleaned_su_gdf.columns.str.startswith(\"GSU25\")])][cleaned_su_gdf[\"UNIQUE_ID\"]==\"FRANKLIN-BAV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d3332-263e-48bd-8890-54b2d1ae67ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "su_pber_splits_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9c52a-69d4-439f-8483-bda1b37df751",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_su_gdf[list(cleaned_su_gdf.columns[cleaned_su_gdf.columns.str.startswith(\"GSU03\")])][cleaned_su_gdf[\"UNIQUE_ID\"]==\"FRANKLIN-BAV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a6afc-a792-4ee9-8f92-9cd2d421df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "su_splits_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271fa2c-eda0-4587-880f-bc6d8d7a14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_su_gdf[(cleaned_su_gdf[\"DIST_len_assignment\"]==0)&(cleaned_su_gdf[\"DIST\"]!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca29ad-fcd5-4b14-aeb3-c7cb0bc14cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "su_splits_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c94f8e-36e5-4457-afa9-1b94da5577dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dict_su = get_prec_to_dist_list_dict(su_pber_splits_gdf, \"GSU\").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d2940-2bce-4457-bde5-b00c7a01edbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_dict_su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489d790-2756-4596-8def-6d04f970696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "su_pber_splits = get_df_with_split_prec_indicator(merge_su, \"GSU\")\n",
    "su_pber_splits_gdf = su_pber_splits[0]\n",
    "su_pber_splits_gdf[\"SU_DIST\"] = 0\n",
    "su_pber_splits_gdf[\"SU_DIST\"] = su_pber_splits_gdf[\"UNIQUE_ID\"].map(su_pber_splits[1])\n",
    "su_splits_dict = pd.Series(su_pber_splits_gdf[\"SU_DIST\"][su_pber_splits_gdf[\"DIST_len_assignment\"]>1].values, index = su_pber_splits_gdf[\"UNIQUE_ID\"][su_pber_splits_gdf[\"DIST_len_assignment\"]>1]).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02aac0-8852-436e-9463-af7660eb6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "su_pber_splits[1][su_pber_splits[1]!=0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f82c0-2109-4efd-8df5-480bc4337dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DIST_index\"].loc[5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c22d5a-c178-4bf9-b953-6786a19177c5",
   "metadata": {},
   "source": [
    "# PICK BACK UP HERE FOR DISTRICT DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121644e-4f26-4fb9-9f4f-cc3e58fd213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_df_with_split_prec_indicator(df, contest):\n",
    "df = merge_su.copy()\n",
    "contest=\"GSU\"\n",
    "t = time.process_time()\n",
    "\n",
    "df[\"DIST\"] = df[\"UNIQUE_ID\"].map(get_prec_to_dist_list_dict(df, contest))\n",
    "\n",
    "df[\"DIST_index\"]=0\n",
    "df[\"DIST_len_assignment\"]=0\n",
    "index_to_set_dict = {}\n",
    "index_to_len_dict = {}\n",
    "\n",
    "#8933=Len of gdf outside of ZZZ precs//keeping as much as possible out of for loop for speed\n",
    "#CHANGE RANGE LEN\n",
    "for i in range(len(df)): #Modify to be len gdf instead of len != 0\n",
    "\n",
    "    #FIX DIST_index - should have assignemnt everywhere\n",
    "    df[\"DIST_index\"].loc[i] = i\n",
    "    if df.loc[i,\"DIST\"]==0:\n",
    "        district_list = [0]\n",
    "        index_to_len_dict[i] = 0\n",
    "    else:\n",
    "        district_list = list(set(df.loc[i,\"DIST\"].split(\", \")))\n",
    "        index_to_len_dict[i] = len(district_list)\n",
    "    index_to_set_dict[i] = district_list\n",
    "\n",
    "    if len(district_list)==1:\n",
    "        district_item = district_list[0]\n",
    "        index_to_set_dict[i] = district_item\n",
    "df[\"DIST_set\"] = df[\"DIST_index\"].map(index_to_set_dict)\n",
    "\n",
    "df[\"DIST_len_assignment\"] = df[\"DIST_index\"].map(index_to_len_dict)\n",
    "prec_to_dist_dict = pd.Series(df[\"DIST_set\"].values, index = df[\"UNIQUE_ID\"])\n",
    "\n",
    "elapsed_time = time.process_time() - t\n",
    "print(\"time to run fxn: \", elapsed_time)\n",
    "    \n",
    "#    return df, prec_to_dist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b82030-d568-41e9-8309-507be25e47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DIST_set\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545c506-ebc8-4252-8e8c-945b367c9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SU_DIST\"] = df[\"UNIQUE_ID\"].map(prec_to_dist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc999b54-c04f-457e-af90-201bea417c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df[\"DIST_len_assignment\"]>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6851f-c2b4-4255-a6ae-6fa8bbd7344d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"DIST_index\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782e7ec-23a8-45ec-9c24-e0c1e07480b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_splits = district_splits_comb(\"SU\", prec_to_dist_dict, df, su_shp, \"UNIQUE_ID\", \"DISTRICT\", list(merge.columns[merge.columns.str.contains(\"GSU\")]), \"SU_DIST\", fill_level = 2)\n",
    "df_elections_gdf = df_splits[0]\n",
    "df_lost_votes_gdf = df_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b09202-e226-41ec-b846-19c4f5161c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = clean_na_dist_assignments(df_elections_gdf, su_shp, \"UNIQUE_ID\", \"SU_DIST\", \"DISTRICT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78cdab-d743-4290-b505-af96a1c5dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[\"SLDU_DIST\"] = df_cleaned[\"SU_DIST\"]\n",
    "df_cleaned = df_cleaned.drop(\"SU_DIST\", axis=1)\n",
    "assert(not(district_assignment_errors(df_cleaned, \"SLDU_DIST\"))), \"Bad SLDU_DIST assignment\"\n",
    "print(\"CONFIRMED: Actual su votes match SLDU_DIST assignment\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306a4db-c93a-4856-b51f-4bee6276cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.drop(\"SU_DIST\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc0827-79b9-4d09-ab3b-efa3c61332e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526cc31-2450-4eff-9629-38512cb9eca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.Series(df[\"SU_DIST\"][df[\"DIST_len_assignment\"]>1].values, index = df[\"UNIQUE_ID\"][df[\"DIST_len_assignment\"]>1]).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919eb350-ab60-449c-9acf-b0423adf73e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prec_to_dist_dict.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55b0be-b6a9-4478-b3e5-d8b33bd7929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(su_pber_splits_gdf[su_pber_splits_gdf[\"DIST\"]!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad0a58-f85d-4839-b466-5649b867e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contest_dist_dict(df, contest):\n",
    "    if contest ==\"GCON\":\n",
    "        col_to_dist_dict = pd.Series(df.columns[df.columns.str.startswith(contest)].str.slice(start=4,stop=6).values, index=df.columns[df.columns.str.startswith(contest)]).to_dict()\n",
    "    elif contest ==\"GSL\":\n",
    "        col_to_dist_dict = pd.Series(df.columns[df.columns.str.startswith(contest)].str.slice(start=3,stop=6).values, index=df.columns[df.columns.str.startswith(contest)]).to_dict()\n",
    "    elif contest ==\"GSU\":\n",
    "        col_to_dist_dict = pd.Series(df.columns[df.columns.str.startswith(contest)].str.slice(start=3,stop=5).values, index=df.columns[df.columns.str.startswith(contest)]).to_dict()\n",
    "    \n",
    "    return col_to_dist_dict\n",
    "\n",
    "\n",
    "def get_prec_to_dist_list_dict(df, contest):\n",
    "    col_dist_dict = get_contest_dist_dict(df, contest)\n",
    "    df[\"DIST\"] = 0\n",
    "    #Instead of iterating over columns, iterate over districts?\n",
    "    for col in df.columns[df.columns.str.startswith(contest)]:\n",
    "        df.loc[(df[col]>0)&(df[\"DIST\"]==0), \"DIST\"] = col_dist_dict.get(col)\n",
    "\n",
    "        df.loc[(df[col]>0)&(df[\"DIST\"]!=0), \"DIST\"] = df[\"DIST\"][(df[col]>0)]+\", \"+col_dist_dict.get(col)\n",
    "    \n",
    "    dist_list_dict = pd.Series(df[\"DIST\"].values, index=df[\"UNIQUE_ID\"])\n",
    "    \n",
    "    return dist_list_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_df_with_split_prec_indicator(df, contest):\n",
    "    t = time.process_time()\n",
    "    \n",
    "    df[\"DIST\"] = df[\"UNIQUE_ID\"].map(get_prec_to_dist_list_dict(df, contest))\n",
    "    \n",
    "    df[\"DIST_index\"]=0\n",
    "    df[\"DIST_len_assignment\"]=0\n",
    "    index_to_set_dict = {}\n",
    "    index_to_len_dict = {}\n",
    "\n",
    "    #8933=Len of gdf outside of ZZZ precs//keeping as much as possible out of for loop for speed\n",
    "    for i in range(0,len(df[df[\"DIST\"]!=0])-1):\n",
    "        df[\"DIST_index\"].loc[i] = i\n",
    "        if df.loc[i,\"DIST\"]==0:\n",
    "            district_list = [0]\n",
    "            index_to_len_dict[i] = 0\n",
    "        else:\n",
    "            district_list = list(set(df.loc[i,\"DIST\"].split(\", \")))\n",
    "            index_to_len_dict[i] = len(district_list)\n",
    "        index_to_set_dict[i] = district_list\n",
    "        \n",
    "        if len(district_list)==1:\n",
    "            district_item = district_list[0]\n",
    "            index_to_set_dict[i] = district_item\n",
    "    df[\"DIST_set\"] = df[\"DIST_index\"].map(index_to_set_dict)\n",
    "   \n",
    "    df[\"DIST_len_assignment\"] = df[\"DIST_index\"].map(index_to_len_dict)\n",
    "    prec_to_dist_dict = pd.Series(df[\"DIST_set\"].values, index = df[\"UNIQUE_ID\"])\n",
    "    \n",
    "    elapsed_time = time.process_time() - t\n",
    "    print(\"time to run fxn: \", elapsed_time)\n",
    "    \n",
    "    return df, prec_to_dist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fb9ba-a183-4e2d-8b1e-5e8cdc0b4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fxns of interest for this issue:\n",
    "def get_df_with_split_prec_indicator(df, contest):\n",
    "    t = time.process_time()\n",
    "    \n",
    "    df[\"DIST\"] = df[\"UNIQUE_ID\"].map(get_prec_to_dist_list_dict(df, contest))\n",
    "    \n",
    "    df[\"DIST_index\"]=0\n",
    "    df[\"DIST_len_assignment\"]=0\n",
    "    index_to_set_dict = {}\n",
    "    index_to_len_dict = {}\n",
    "\n",
    "    #8933=Len of gdf outside of ZZZ precs//keeping as much as possible out of for loop for speed\n",
    "    for i in range(0,len(df[df[\"DIST\"]!=0])-1):\n",
    "        df[\"DIST_index\"].loc[i] = i\n",
    "        if df.loc[i,\"DIST\"]==0:\n",
    "            district_list = [0]\n",
    "            index_to_len_dict[i] = 0\n",
    "        else:\n",
    "            district_list = list(set(df.loc[i,\"DIST\"].split(\", \")))\n",
    "            index_to_len_dict[i] = len(district_list)\n",
    "        index_to_set_dict[i] = district_list\n",
    "        \n",
    "        if len(district_list)==1:\n",
    "            district_item = district_list[0]\n",
    "            index_to_set_dict[i] = district_item\n",
    "    df[\"DIST_set\"] = df[\"DIST_index\"].map(index_to_set_dict)\n",
    "   \n",
    "    df[\"DIST_len_assignment\"] = df[\"DIST_index\"].map(index_to_len_dict)\n",
    "    prec_to_dist_dict = pd.Series(df[\"DIST_set\"].values, index = df[\"UNIQUE_ID\"])\n",
    "    \n",
    "    elapsed_time = time.process_time() - t\n",
    "    print(\"time to run fxn: \", elapsed_time)\n",
    "    \n",
    "    return df, prec_to_dist_dict\n",
    "\n",
    "\n",
    "def generate_differences_df(df_compare_against, df_compare_to, unique_ID_col, races_list, drop_empty = False):\n",
    "    \n",
    "    df_compare_against = df_compare_against[[unique_ID_col]+races_list]\n",
    "    df_compare_to = df_compare_to[[unique_ID_col]+races_list]\n",
    " \n",
    "    grouped_compare_against = df_compare_against.groupby(unique_ID_col).sum()\n",
    "    grouped_compare_to = df_compare_to.groupby(unique_ID_col).sum()\n",
    "    \n",
    "    grouped_compare_against.reset_index(inplace = True, drop = False)\n",
    "    grouped_compare_to.reset_index(inplace = True, drop = False)\n",
    "    diffs = grouped_compare_against.set_index(unique_ID_col).subtract(grouped_compare_to.set_index(unique_ID_col))\n",
    "    \n",
    "    diffs[\"Tot_Votes\"] = diffs[races_list].sum(axis=1)\n",
    "    \n",
    "    if drop_empty:\n",
    "        diffs = diffs.loc[~(diffs==0).all(axis=1)]\n",
    "        diffs = diffs.loc[:, (diffs != 0).any(axis=0)]\n",
    "    return diffs\n",
    "\n",
    "\n",
    "def district_splits_comb(level, splits_dict, elections_gdf, district_gdf, unique_ID_col, district_gdf_ID, races_list, elections_gdf_dist_ID, fill_level = 2):\n",
    "    '''\n",
    "    Function to split precincts across districts that splits a precinct across the entire district map.\n",
    "    Previous iterations of this code only split precincts by the districts in which votes were recorded.\n",
    "    In some instances, that led to holes in the map, due to districts where no votes were recorded in a precinct, but where an intersection occurred.\n",
    "    '''\n",
    "    t = time.process_time()\n",
    "    # Intersect the elections gdf with the district gdf\n",
    "    need_splits = elections_gdf[elections_gdf[unique_ID_col].isin(list(splits_dict.keys()))]\n",
    "    others = elections_gdf[~elections_gdf[unique_ID_col].isin(list(splits_dict.keys()))]\n",
    "    \n",
    "    pre_splits_copy = need_splits.copy(deep = True)\n",
    "    \n",
    "    test_join = gp.overlay(need_splits, district_gdf, how = \"intersection\")\n",
    "    \n",
    "    # Assign a district column, using the district shapefile\n",
    "    test_join[elections_gdf_dist_ID] = test_join[district_gdf_ID]\n",
    "    \n",
    "    # Filter the intersection down to the precinct, district pairs we need\n",
    "    clean_votes = test_join.copy(deep = True)\n",
    "    \n",
    "    clean_votes[unique_ID_col+\"_new\"] = clean_votes[unique_ID_col]\n",
    "    \n",
    "    # Remove the others and hold on to these to be merged later\n",
    "    for index, row in clean_votes.iterrows():\n",
    "        clean_votes.at[index, unique_ID_col+\"_new\"] = row[unique_ID_col]+\"-(\"+level + \"-\" + row[district_gdf_ID].zfill(fill_level) + \")\" \n",
    "        for column in test_join:\n",
    "            if column in races_list and row[elections_gdf_dist_ID].zfill(fill_level) not in column:\n",
    "                clean_votes.at[index, column] = 0 \n",
    "        \n",
    "    lost_votes_df = generate_differences_df(pre_splits_copy, clean_votes, unique_ID_col, races_list, True)\n",
    "    \n",
    "    clean_votes.drop(unique_ID_col, axis = 1, inplace = True)\n",
    "    clean_votes.rename(columns = {unique_ID_col+\"_new\":unique_ID_col}, inplace = True)\n",
    "    clean_votes = clean_votes[list(others.columns)]\n",
    "        \n",
    "    elections_gdf = gp.GeoDataFrame(pd.concat([clean_votes, others]), crs = elections_gdf.crs)\n",
    "    elections_gdf.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    elapsed_time = time.process_time() - t\n",
    "    print(\"time to run fxn: \", elapsed_time)\n",
    "    \n",
    "    return elections_gdf, lost_votes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e634c0a7-6289-436d-870f-31e58f58ad85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdv_env",
   "language": "python",
   "name": "hdv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
